{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "707416f1",
   "metadata": {},
   "source": [
    "# 1) Setup\n",
    "We will be using different libraries and modules which have to be (pip) installed or imported. You can find a list of both below, sorted by their first appearance in the code.\n",
    "## 1.1) pip install commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e411dc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -aleido (c:\\users\\maxim\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Invalid requirement: '#'\n",
      "WARNING: Ignoring invalid distribution -aleido (c:\\users\\maxim\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -aleido (c:\\users\\maxim\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -aleido (c:\\users\\maxim\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Section 5.1)\n",
    "!pip install Keras-Preprocessing # '!' runs pip as a shell rather than a notebook command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c003d40",
   "metadata": {},
   "source": [
    "## 1.2) Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d528bec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Section 2.1) Set Folder Path\n",
    "import os\n",
    "\n",
    "# Section 2.2) Import air quality data for New York, Milan and Tokyo from data folder\n",
    "import pandas as pd\n",
    "\n",
    "# Section 2.3) Import New York health data from Github repository and do a quality check\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# Section 3.3) Process final dataframe\n",
    "import numpy as np\n",
    "\n",
    "# Section 4.1) Scatterplots\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Section 4.2) Plotting by date\n",
    "import math\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Section 5.2) Create functions for training and testing\n",
    "from tensorflow.keras import models, layers, utils, backend\n",
    "import tensorflow as tf\n",
    " \n",
    "# Section 5.3) Run the model training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eea88a",
   "metadata": {},
   "source": [
    "## 1.3) Shell commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376bd3d",
   "metadata": {},
   "source": [
    "To use the *write_image()* function in section 4.2 with the 'kaleido' engine, please run the following command in the Windows Command Prompt: \n",
    "<br> pip install -U kaleido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57fa2c2",
   "metadata": {},
   "source": [
    "## 1.4) Set seed\n",
    "For better reproducibility, we are setting a global seed. We use *tf.keras.utils.set_random_seed()* instead of *tf.random.set_seed()* as the former affects all standard Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8909c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 777\n",
    "tf.keras.utils.set_random_seed(seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b2c18",
   "metadata": {},
   "source": [
    "# 2) Import Data\n",
    "## 2.1) Set Folder Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f162fdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim\\OneDrive\\Desktop\\Unibo\\Machine Learning Systems for Data Science_90477\\Module II\\Final Project\\pr7-main\\scripts\n"
     ]
    }
   ],
   "source": [
    "PATH = '..\\data'\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a07384",
   "metadata": {},
   "source": [
    "## 2.2) Import air quality data for New York, Milan and Tokyo from data folder\n",
    "\n",
    "Here we import the air quality data for the three cities from our folder path. We rename the pollutants by removing a space so they work seamlessly in our code. We then get an overview of our data: In the *describe()* function we set *datetime_is_numeric = True*, so datetime data is treated as numeric rather than categorical, which also silences a Python warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "746455a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3207 entries, 0 to 3206\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   date     3207 non-null   datetime64[ns]\n",
      " 1   pm25_ny  3207 non-null   object        \n",
      " 2   o3_ny    3207 non-null   object        \n",
      " 3   no2_ny   3207 non-null   object        \n",
      " 4   co_ny    3207 non-null   object        \n",
      "dtypes: datetime64[ns](1), object(4)\n",
      "memory usage: 125.4+ KB\n",
      "        date pm25_ny o3_ny no2_ny co_ny\n",
      "0 2022-11-01      40    28     20     3\n",
      "1 2022-11-02      45    23     25     3\n",
      "2 2022-11-03      53    17     26     4\n",
      "3 2022-11-04      47    24     10     1\n",
      "4 2022-11-05      31    21      6     1\n"
     ]
    }
   ],
   "source": [
    "# 2.2.1) For New York\n",
    "ny_air_PATH = os.path.join(PATH, 'new-york-air-quality.csv')\n",
    "ny_air = pd.read_csv(ny_air_PATH, parse_dates = ['date'])\n",
    "ny_air.rename(columns = {' pm25': 'pm25_ny', ' o3': 'o3_ny', ' no2': 'no2_ny', ' co': 'co_ny'}, inplace = True)\n",
    "\n",
    "# Overview\n",
    "ny_air.info()\n",
    "ny_air.describe(datetime_is_numeric = True)\n",
    "print(ny_air.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c635343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2860 entries, 0 to 2859\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   date     2860 non-null   datetime64[ns]\n",
      " 1   pm25_ml  2860 non-null   object        \n",
      " 2   pm10_ml  2860 non-null   object        \n",
      " 3   no2_ml   2860 non-null   object        \n",
      " 4   co_ml    2860 non-null   object        \n",
      "dtypes: datetime64[ns](1), object(4)\n",
      "memory usage: 111.8+ KB\n",
      "        date pm25_ml pm10_ml no2_ml co_ml\n",
      "0 2022-12-01     134      34     27      \n",
      "1 2022-12-02      82      24     23      \n",
      "2 2022-12-03      76      13     20      \n",
      "3 2022-12-04      53      16     22      \n",
      "4 2022-12-05      50      36     39      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim\\AppData\\Local\\Temp\\ipykernel_22752\\1604748629.py:9: FutureWarning: Treating datetime data as categorical rather than numeric in `.describe` is deprecated and will be removed in a future version of pandas. Specify `datetime_is_numeric=True` to silence this warning and adopt the future behavior now.\n",
      "  ml_air.describe()\n"
     ]
    }
   ],
   "source": [
    "# 2.2.2) For Milan\n",
    "ml_air_PATH = os.path.join(PATH, 'milano-senato__lombardia__italy-air-quality.csv')\n",
    "ml_air = pd.read_csv(ml_air_PATH, parse_dates = ['date'])\n",
    "ml_air.rename(columns = {' pm25': 'pm25_ml', ' pm10': 'pm10_ml', ' no2': 'no2_ml',\\\n",
    "                         ' co': 'co_ml'}, inplace = True)\n",
    "\n",
    "# Overview\n",
    "ml_air.info()\n",
    "ml_air.describe()\n",
    "print(ml_air.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dbe3847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3271 entries, 0 to 3270\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   date     3271 non-null   datetime64[ns]\n",
      " 1   pm25_tk  3271 non-null   object        \n",
      " 2   pm10_tk  3271 non-null   object        \n",
      " 3   o3_tk    3271 non-null   object        \n",
      " 4   no2_tk   3271 non-null   object        \n",
      " 5   so2_tk   3271 non-null   object        \n",
      " 6   co_tk    3271 non-null   object        \n",
      "dtypes: datetime64[ns](1), object(6)\n",
      "memory usage: 179.0+ KB\n",
      "        date pm25_tk pm10_tk o3_tk no2_tk so2_tk co_tk\n",
      "0 2022-12-01      25       9    19     15      1     2\n",
      "1 2022-12-02      29      11           18            4\n",
      "2 2022-12-03      36      14    15     16      1     4\n",
      "3 2022-12-04      46       9    17     16      1     2\n",
      "4 2022-12-05      29       7    14     17            3\n"
     ]
    }
   ],
   "source": [
    "# 2.2.3) For Tokyo\n",
    "tk_air_PATH = os.path.join(PATH, 'hibiyakoen_-chiyoda__tokyo__japan-air-quality.csv')\n",
    "tk_air = pd.read_csv(tk_air_PATH, parse_dates = ['date'])\n",
    "tk_air.rename(columns = {' pm25': 'pm25_tk', ' pm10': 'pm10_tk', ' o3': 'o3_tk',\\\n",
    "                         ' no2': 'no2_tk', ' so2': 'so2_tk', ' co': 'co_tk'}, inplace = True)\n",
    "\n",
    "# Overview\n",
    "tk_air.info()\n",
    "tk_air.describe(datetime_is_numeric = True)\n",
    "print(tk_air.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11cc360",
   "metadata": {},
   "source": [
    "## 2.3) Import New York health data from Github repository and do a quality check\n",
    "\n",
    "Here we import the health data for New York from a Github repository, using the request package and the 'raw' version of the URL. The code *requests.get().content* outputs a byte object, which can be decoded using *io.StringIO*. The resulting *io.StringIO* object is accepted as input for the *pd.read_csv()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f59e1f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1072 entries, 0 to 1071\n",
      "Data columns (total 67 columns):\n",
      " #   Column                           Non-Null Count  Dtype         \n",
      "---  ------                           --------------  -----         \n",
      " 0   date_of_interest                 1072 non-null   datetime64[ns]\n",
      " 1   CASE_COUNT                       1072 non-null   int64         \n",
      " 2   PROBABLE_CASE_COUNT              1072 non-null   int64         \n",
      " 3   HOSPITALIZED_COUNT               1072 non-null   int64         \n",
      " 4   death_ny                         1072 non-null   int64         \n",
      " 5   PROBABLE_DEATH_COUNT             1072 non-null   int64         \n",
      " 6   CASE_COUNT_7DAY_AVG              1072 non-null   int64         \n",
      " 7   ALL_CASE_COUNT_7DAY_AVG          1072 non-null   int64         \n",
      " 8   HOSP_COUNT_7DAY_AVG              1072 non-null   int64         \n",
      " 9   DEATH_COUNT_7DAY_AVG             1072 non-null   int64         \n",
      " 10  ALL_DEATH_COUNT_7DAY_AVG         1072 non-null   int64         \n",
      " 11  BX_CASE_COUNT                    1072 non-null   int64         \n",
      " 12  BX_PROBABLE_CASE_COUNT           1072 non-null   int64         \n",
      " 13  BX_HOSPITALIZED_COUNT            1072 non-null   int64         \n",
      " 14  BX_DEATH_COUNT                   1072 non-null   int64         \n",
      " 15  BX_PROBABLE_DEATH_COUNT          1072 non-null   int64         \n",
      " 16  BX_CASE_COUNT_7DAY_AVG           1072 non-null   int64         \n",
      " 17  BX_PROBABLE_CASE_COUNT_7DAY_AVG  1072 non-null   int64         \n",
      " 18  BX_ALL_CASE_COUNT_7DAY_AVG       1072 non-null   int64         \n",
      " 19  BX_HOSPITALIZED_COUNT_7DAY_AVG   1072 non-null   int64         \n",
      " 20  BX_DEATH_COUNT_7DAY_AVG          1072 non-null   int64         \n",
      " 21  BX_ALL_DEATH_COUNT_7DAY_AVG      1072 non-null   int64         \n",
      " 22  BK_CASE_COUNT                    1072 non-null   int64         \n",
      " 23  BK_PROBABLE_CASE_COUNT           1072 non-null   int64         \n",
      " 24  BK_HOSPITALIZED_COUNT            1072 non-null   int64         \n",
      " 25  BK_DEATH_COUNT                   1072 non-null   int64         \n",
      " 26  BK_PROBABLE_DEATH_COUNT          1072 non-null   int64         \n",
      " 27  BK_CASE_COUNT_7DAY_AVG           1072 non-null   int64         \n",
      " 28  BK_PROBABLE_CASE_COUNT_7DAY_AVG  1072 non-null   int64         \n",
      " 29  BK_ALL_CASE_COUNT_7DAY_AVG       1072 non-null   int64         \n",
      " 30  BK_HOSPITALIZED_COUNT_7DAY_AVG   1072 non-null   int64         \n",
      " 31  BK_DEATH_COUNT_7DAY_AVG          1072 non-null   int64         \n",
      " 32  BK_ALL_DEATH_COUNT_7DAY_AVG      1072 non-null   int64         \n",
      " 33  MN_CASE_COUNT                    1072 non-null   int64         \n",
      " 34  MN_PROBABLE_CASE_COUNT           1072 non-null   int64         \n",
      " 35  MN_HOSPITALIZED_COUNT            1072 non-null   int64         \n",
      " 36  MN_DEATH_COUNT                   1072 non-null   int64         \n",
      " 37  MN_PROBABLE_DEATH_COUNT          1072 non-null   int64         \n",
      " 38  MN_CASE_COUNT_7DAY_AVG           1072 non-null   int64         \n",
      " 39  MN_PROBABLE_CASE_COUNT_7DAY_AVG  1072 non-null   int64         \n",
      " 40  MN_ALL_CASE_COUNT_7DAY_AVG       1072 non-null   int64         \n",
      " 41  MN_HOSPITALIZED_COUNT_7DAY_AVG   1072 non-null   int64         \n",
      " 42  MN_DEATH_COUNT_7DAY_AVG          1072 non-null   int64         \n",
      " 43  MN_ALL_DEATH_COUNT_7DAY_AVG      1072 non-null   int64         \n",
      " 44  QN_CASE_COUNT                    1072 non-null   int64         \n",
      " 45  QN_PROBABLE_CASE_COUNT           1072 non-null   int64         \n",
      " 46  QN_HOSPITALIZED_COUNT            1072 non-null   int64         \n",
      " 47  QN_DEATH_COUNT                   1072 non-null   int64         \n",
      " 48  QN_PROBABLE_DEATH_COUNT          1072 non-null   int64         \n",
      " 49  QN_CASE_COUNT_7DAY_AVG           1072 non-null   int64         \n",
      " 50  QN_PROBABLE_CASE_COUNT_7DAY_AVG  1072 non-null   int64         \n",
      " 51  QN_ALL_CASE_COUNT_7DAY_AVG       1072 non-null   int64         \n",
      " 52  QN_HOSPITALIZED_COUNT_7DAY_AVG   1072 non-null   int64         \n",
      " 53  QN_DEATH_COUNT_7DAY_AVG          1072 non-null   int64         \n",
      " 54  QN_ALL_DEATH_COUNT_7DAY_AVG      1072 non-null   int64         \n",
      " 55  SI_CASE_COUNT                    1072 non-null   int64         \n",
      " 56  SI_PROBABLE_CASE_COUNT           1072 non-null   int64         \n",
      " 57  SI_HOSPITALIZED_COUNT            1072 non-null   int64         \n",
      " 58  SI_DEATH_COUNT                   1072 non-null   int64         \n",
      " 59  SI_PROBABLE_DEATH_COUNT          1072 non-null   int64         \n",
      " 60  SI_CASE_COUNT_7DAY_AVG           1072 non-null   int64         \n",
      " 61  SI_PROBABLE_CASE_COUNT_7DAY_AVG  1072 non-null   int64         \n",
      " 62  SI_ALL_CASE_COUNT_7DAY_AVG       1072 non-null   int64         \n",
      " 63  SI_HOSPITALIZED_COUNT_7DAY_AVG   1072 non-null   int64         \n",
      " 64  SI_DEATH_COUNT_7DAY_AVG          1072 non-null   int64         \n",
      " 65  SI_ALL_DEATH_COUNT_7DAY_AVG      1072 non-null   int64         \n",
      " 66  INCOMPLETE                       1072 non-null   int64         \n",
      "dtypes: datetime64[ns](1), int64(66)\n",
      "memory usage: 561.2 KB\n",
      "  date_of_interest  CASE_COUNT  PROBABLE_CASE_COUNT  HOSPITALIZED_COUNT  \\\n",
      "0       2020-02-29           1                    0                   1   \n",
      "1       2020-03-01           0                    0                   1   \n",
      "2       2020-03-02           0                    0                   2   \n",
      "3       2020-03-03           1                    0                   7   \n",
      "4       2020-03-04           5                    0                   2   \n",
      "\n",
      "   death_ny  PROBABLE_DEATH_COUNT  CASE_COUNT_7DAY_AVG  \\\n",
      "0         0                     0                    0   \n",
      "1         0                     0                    0   \n",
      "2         0                     0                    0   \n",
      "3         0                     0                    0   \n",
      "4         0                     0                    0   \n",
      "\n",
      "   ALL_CASE_COUNT_7DAY_AVG  HOSP_COUNT_7DAY_AVG  DEATH_COUNT_7DAY_AVG  ...  \\\n",
      "0                        0                    0                     0  ...   \n",
      "1                        0                    0                     0  ...   \n",
      "2                        0                    0                     0  ...   \n",
      "3                        0                    0                     0  ...   \n",
      "4                        0                    0                     0  ...   \n",
      "\n",
      "   SI_HOSPITALIZED_COUNT  SI_DEATH_COUNT  SI_PROBABLE_DEATH_COUNT  \\\n",
      "0                      0               0                        0   \n",
      "1                      0               0                        0   \n",
      "2                      0               0                        0   \n",
      "3                      0               0                        0   \n",
      "4                      0               0                        0   \n",
      "\n",
      "   SI_CASE_COUNT_7DAY_AVG  SI_PROBABLE_CASE_COUNT_7DAY_AVG  \\\n",
      "0                       0                                0   \n",
      "1                       0                                0   \n",
      "2                       0                                0   \n",
      "3                       0                                0   \n",
      "4                       0                                0   \n",
      "\n",
      "   SI_ALL_CASE_COUNT_7DAY_AVG  SI_HOSPITALIZED_COUNT_7DAY_AVG  \\\n",
      "0                           0                               0   \n",
      "1                           0                               0   \n",
      "2                           0                               0   \n",
      "3                           0                               0   \n",
      "4                           0                               0   \n",
      "\n",
      "   SI_DEATH_COUNT_7DAY_AVG  SI_ALL_DEATH_COUNT_7DAY_AVG  INCOMPLETE  \n",
      "0                        0                            0           0  \n",
      "1                        0                            0           0  \n",
      "2                        0                            0           0  \n",
      "3                        0                            0           0  \n",
      "4                        0                            0           0  \n",
      "\n",
      "[5 rows x 67 columns]\n"
     ]
    }
   ],
   "source": [
    "# 2.3.1) Import New York health data from Github repository\n",
    "url_ny_health = 'https://raw.githubusercontent.com/nychealth/coronavirus-data/master/trends/data-by-day.csv'\n",
    "download_ny_health = requests.get(url_ny_health).content # class: 'bytes'\n",
    "ny_health = pd.read_csv(io.StringIO(download_ny_health.decode('utf-8')), sep = ',', parse_dates = ['date_of_interest'])\n",
    "ny_health.rename(columns = {'DEATH_COUNT': 'death_ny'}, inplace = True)\n",
    "\n",
    "# Overview\n",
    "ny_health.info()\n",
    "ny_health.describe(datetime_is_numeric = True)\n",
    "print(ny_health.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b3e41",
   "metadata": {},
   "source": [
    "As we can see, the data includes separate *CASE_COUNT* variables for the districts of New York city. In the following cell, we check if the total death count is really the sum of the district death counts. We see that this is not the case on two dates. However, these two dates do not fall into our focus period, and can therefore be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ae78610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_of_interest</th>\n",
       "      <th>death_ny</th>\n",
       "      <th>death_ny_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>2021-04-19</td>\n",
       "      <td>42</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>2021-12-13</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    date_of_interest  death_ny  death_ny_sum\n",
       "415       2021-04-19        42            41\n",
       "653       2021-12-13        21            20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.3.2) Compare the confirmed death count of all districts with the aggregate variable (quality check)\n",
    "ny_health_death_sum = pd.DataFrame({'death_ny_sum': ny_health.loc[:, ['BX_DEATH_COUNT', 'BK_DEATH_COUNT', 'MN_DEATH_COUNT',\\\n",
    "                                                                   'QN_DEATH_COUNT', 'SI_DEATH_COUNT']].sum(axis = 1)})\n",
    "ny_health_death_total = ny_health[['date_of_interest', 'death_ny']]\n",
    "ny_health_death_merged = pd.concat([ny_health_death_total, ny_health_death_sum], axis = 1)\n",
    "\n",
    "\"\"\"We use the merged dataframe to: \n",
    "1) get a simple boolean response if there is a mismatch\n",
    "2) show the exact rows where the mismatch appears\"\"\"\n",
    "\n",
    "print(ny_health_death_merged['death_ny'].equals(ny_health_death_merged['death_ny_sum']))\n",
    "\n",
    "\"\"\"We do not have a perfect match. However, the mismatch is negligible as it does not fall into the period (2020-03-03 - \n",
    "2020-06-26) we are focusing on:\"\"\"\n",
    "ny_health_death_merged.loc[ny_health_death_merged['death_ny'] != ny_health_death_merged['death_ny_sum']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2921a",
   "metadata": {},
   "source": [
    "The quality check showed that *death_ny* can be used for our study. We now throw out all the other variables and keep only *date_of_interest* and *death_ny*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15e95458",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_health = ny_health.loc[:, ['date_of_interest', 'death_ny']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f324e",
   "metadata": {},
   "source": [
    "## 2.4) Import Milan health data from Github repository and derive a proxy for the death count\n",
    "\n",
    "The Milan health data is also imported from a Github repository, using the request package and the 'raw' version of the URL. However, we have a separate CSV file for every day, and each CSV file shows data for all the provinces of Italy. The approach is to loop through every day in our focus period, create an URL by concatenation, and then extract the value under for the *Milano* row under the *totale_casi* column. This is achieved by string splitting the content of our *request.get()* output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d899c684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': ['20200302', '20200303', '20200304', '20200305', '20200306', '20200307', '20200308', '20200309', '20200310', '20200311', '20200312', '20200313', '20200314', '20200315', '20200316', '20200317', '20200318', '20200319', '20200320', '20200321', '20200322', '20200323', '20200324', '20200325', '20200326', '20200327', '20200328', '20200329', '20200330', '20200331', '20200401', '20200402', '20200403', '20200404', '20200405', '20200406', '20200407', '20200408', '20200409', '20200410', '20200411', '20200412', '20200413', '20200414', '20200415', '20200416', '20200417', '20200418', '20200419', '20200420', '20200421', '20200422', '20200423', '20200424', '20200425', '20200426', '20200427', '20200428', '20200429', '20200430', '20200501', '20200502', '20200503', '20200504', '20200505', '20200506', '20200507', '20200508', '20200509', '20200510', '20200511', '20200512', '20200513', '20200514', '20200515', '20200516', '20200517', '20200518', '20200519', '20200520', '20200521', '20200522', '20200523', '20200524', '20200525', '20200526', '20200527', '20200528', '20200529', '20200530', '20200531', '20200601', '20200602', '20200603', '20200604', '20200605', '20200606', '20200607', '20200608', '20200609', '20200610', '20200611', '20200612', '20200613', '20200614', '20200615', '20200616', '20200617', '20200618', '20200619', '20200620', '20200621', '20200622', '20200623', '20200624', '20200625', '20200626'], 'cum_case_ml': ['58', '93', '145', '197', '267', '361', '406', '506', '592', '925', '1146', '1307', '1551', '1750', '1983', '2326', '2644', '3278', '3804', '4672', '5096', '5326', '5701', '6074', '6922', '7469', '7783', '8329', '8676', '8911', '9522', '10004', '10391', '10819', '11230', '11538', '11787', '12039', '12479', '12748', '13268', '13680', '14161', '14350', '14675', '14952', '15277', '15546', '15825', '16112', '16520', '17000', '17277', '17689', '17908', '18371', '18559', '18837', '19121', '19337', '19701', '19950', '20068', '20254', '20398', '20711', '20893', '21094', '21272', '21376', '21490', '21626', '21731', '21900', '21966', '22041', '22151', '22222', '22324', '22372', '22455', '22528', '22616', '22680', '22726', '22764', '22832', '22908', '22982', '23044', '23076', '23094', '23139', '23176', '23207', '23306', '23365', '23408', '23437', '23483', '23510', '23581', '23669', '23766', '23811', '23863', '23905', '23966', '24018', '24061', '24130', '24161', '24184', '24210', '24239', '24267', '24300']}\n"
     ]
    }
   ],
   "source": [
    "# 2.4.1) Import Milan health data from Github repository (takes around 40sec to run)\n",
    "\n",
    "# Create a date range for concatenation (below) and initialize the case_count list and the ml_health dictionary\n",
    "# Note: We start on 2nd March 2020 in order to derive the growth (= new cases) for 3rd March 2020.\n",
    "date_range_strings = pd.date_range(start = '20200302', end = '20200626', freq = 'D').strftime('%Y%m%d')\n",
    "case_count_list = []\n",
    "ml_health = {'date': [], 'cum_case_ml': []}\n",
    "\n",
    "# Loop through list of URLs and create a dictionary with key = 'date' and value = 'case'\n",
    "for i in date_range_strings:\n",
    "    \n",
    "    # Create URL by string concatenation\n",
    "    url_ml_health = ('https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-province/dpc-covid19-ita-province-'\\\n",
    "                     + i + '.csv') \n",
    "    download_ml_health = requests.get(url_ml_health).content # class: 'bytes'\n",
    "    \n",
    "    # Use string splits to single out the row for 'Milano' and the column 'totale_casi'\n",
    "    cum_case_count = str(download_ml_health).split('Milano')[1].split(',\\\\n')[0].split(',')[4] \n",
    "    \n",
    "    # Append the dictionary with the new key-value pair\n",
    "    ml_health['date'].append(i)\n",
    "    ml_health['cum_case_ml'].append(cum_case_count)\n",
    "    \n",
    "print(ml_health)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033e1c3",
   "metadata": {},
   "source": [
    "Looking at the metadata (Source: https://github.com/pcm-dpc/COVID-19/blob/master/dati-andamento-covid19-italia.md), the above \n",
    "output for 'totale_casi' only gives us the 'Total amount of positive cases', which is a cumulative sum. We therefore have to take the first difference to obtain the new cases per day. Afterwards, we multiply the number of cases by the case fatality rate (CFR) to obtain a proxy for the death count. The CFR for Milan during February-May 2020 has been calculated as 17.3%. (Source: https://epiprev.it/documenti/downloadfile.php?fileid=29bf488c8dae81773eee15a8497d89b5cdec4f84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb3b411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>cum_case_ml</th>\n",
       "      <th>case_ml</th>\n",
       "      <th>death_ml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200302</td>\n",
       "      <td>58</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200303</td>\n",
       "      <td>93</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200304</td>\n",
       "      <td>145</td>\n",
       "      <td>52</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200305</td>\n",
       "      <td>197</td>\n",
       "      <td>52</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200306</td>\n",
       "      <td>267</td>\n",
       "      <td>70</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date cum_case_ml  case_ml  death_ml\n",
       "0  20200302          58     <NA>      <NA>\n",
       "1  20200303          93       35         6\n",
       "2  20200304         145       52         9\n",
       "3  20200305         197       52         9\n",
       "4  20200306         267       70        12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.4.2) Create a proxy for the death count by multiplying the daily cases with the case fatality rate (CFR)\n",
    "\n",
    "# 2.4.2.1) Convert the ml_health dictionary into a dataframe\n",
    "ml_health = pd.DataFrame.from_dict(ml_health)\n",
    "\n",
    "# 2.4.2.2) Take the first difference to obtain the new cases per day\n",
    "ml_health['case_ml'] = ml_health['cum_case_ml'].astype(pd.Int64Dtype()).diff(periods = 1)\n",
    "\n",
    "# 2.4.2.3) Multiply the number of cases by a CFR of 17.3%\n",
    "cfr_ml = 0.173\n",
    "ml_health['death_ml'] = (ml_health['case_ml'] * cfr_ml).round().astype(pd.Int64Dtype())\n",
    "ml_health.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894a147",
   "metadata": {},
   "source": [
    "## 2.5) Import Tokyo health data from Github repository\n",
    "\n",
    "For Tokyo, we are using again the 'raw' version of the data, which is provided by the Tokyo Metropolitan Government on Github. The data is stored in the JSON format as a dictionary with two keys, the second of which provides a list of dictionaries in its values: We use *read_json()* from pandas and change the parameters of the function to obtain a list of dictionaries, which can be converted to a dataframe using the *DataFrame()* function from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60e7a3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1079 entries, 0 to 1078\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   death_date  1079 non-null   datetime64[ns]\n",
      " 1   death_tk    1079 non-null   int64         \n",
      "dtypes: datetime64[ns](1), int64(1)\n",
      "memory usage: 17.0 KB\n",
      "  death_date  death_tk\n",
      "0 2020-02-26         1\n",
      "1 2020-02-27         0\n",
      "2 2020-02-28         0\n",
      "3 2020-02-29         0\n",
      "4 2020-03-01         0\n"
     ]
    }
   ],
   "source": [
    "url_tk_health = 'https://raw.githubusercontent.com/tokyo-metropolitan-gov/covid19/development/data/deaths.json'\n",
    "\n",
    "# The output of the following pd.json_read() command is a list of dictionaries:\n",
    "tk_health = pd.read_json(url_tk_health, orient = 'index', typ = 'series')['data'] \n",
    "\n",
    "# Turn the list of dictionaries into a dataframe \n",
    "tk_health = pd.DataFrame(tk_health)\n",
    "tk_health.rename(columns = {'count': 'death_tk'}, inplace = True)\n",
    "tk_health['death_date'] = pd.to_datetime(tk_health['death_date'],  format = '%Y-%m-%d')\n",
    "\n",
    "# Overview\n",
    "tk_health.info()\n",
    "tk_health.describe(datetime_is_numeric = True)\n",
    "print(tk_health.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361678f7",
   "metadata": {},
   "source": [
    "# 3) Process and merge data\n",
    "## 3.1) Process data\n",
    "\n",
    "In the next steps, we define and apply two functions which prepare all our dataframes, before merging them to our final dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb93aaf",
   "metadata": {},
   "source": [
    "### 3.1.1) Convert the date column and sort the data by dates; Print the date range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac79304",
   "metadata": {},
   "source": [
    "The function *convert_sort_date_rows()*, creates an index from the date column, converts the date column to a *datetime64[ns]* data type and sorts the data. We also obtain a print of the date range covered by our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3209fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sort_date_rows(data, date_column_name: str):\n",
    "    \n",
    "    # Use date_column_name to create a column index number\n",
    "    date_column_index = data.columns.get_loc(date_column_name)\n",
    "    \n",
    "    # Convert date column to date-format without loosing the Dtype 'datetime64[ns]'\n",
    "    data.iloc[:, date_column_index] = pd.to_datetime(data.iloc[:, date_column_index])\n",
    "    \n",
    "    # Sort by date_column_name and print the date range\n",
    "    data = data.sort_values(by = date_column_name)\n",
    "    date_range_min = min(data.iloc[:, date_column_index].dt.strftime('%Y-%m-%d'))\n",
    "    date_range_max = max(data.iloc[:, date_column_index].dt.strftime('%Y-%m-%d'))\n",
    "    print(f'Date range: {date_range_min}, {date_range_max}')\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83365598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York air quality data:\n",
      "Date range: 2014-01-01, 2022-11-20\n",
      "Tokyo air quality data:\n",
      "Date range: 2014-01-02, 2022-12-31\n",
      "Milan air quality data:\n",
      "Date range: 2014-10-05, 2022-12-28\n"
     ]
    }
   ],
   "source": [
    "# 3.1.2) Apply the convert_sort_date_rows() function to air data\n",
    "print(\"New York air quality data:\")\n",
    "ny_air = convert_sort_date_rows(data = ny_air, date_column_name = 'date')\n",
    "print(\"Tokyo air quality data:\")\n",
    "tk_air = convert_sort_date_rows(data = tk_air, date_column_name = 'date')\n",
    "print(\"Milan air quality data:\")\n",
    "ml_air = convert_sort_date_rows(data = ml_air, date_column_name = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2eaef2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York health data:\n",
      "Date range: 2020-02-29, 2023-02-04\n",
      "Tokyo health data:\n",
      "Date range: 2020-02-26, 2023-02-08\n",
      "Milan health data:\n",
      "Date range: 2020-03-02, 2020-06-26\n"
     ]
    }
   ],
   "source": [
    "# 3.1.3) Apply the convert_sort_date_rows() function to health data\n",
    "print(\"New York health data:\")\n",
    "ny_health = convert_sort_date_rows(data = ny_health, date_column_name = 'date_of_interest')\n",
    "print(\"Tokyo health data:\")\n",
    "tk_health = convert_sort_date_rows(data = tk_health, date_column_name = 'death_date')\n",
    "print(\"Milan health data:\")\n",
    "ml_health = convert_sort_date_rows(data = ml_health, date_column_name = 'date')\n",
    "# We can see that the Tokyo date range includes the other. We therefore need an outer merge in 3.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83043a01",
   "metadata": {},
   "source": [
    "### 3.1.4) Find missing date rows in both air and health and add the missing rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d592f2d0",
   "metadata": {},
   "source": [
    "The function *add_missing_date_rows()* finds missing rows (= dates) in our data and adds them back, which means that some rows are created which show missing values on the other columns. We also get an output of the missing rows before and after the fix.\n",
    "\n",
    "<br> 1) After using the date column to create a column index number as well as the date range minimum & maximum, we use the date column as the new index. Setting the date column as the index of the DataFrame allows us to use the Pandas time-series functionality, such as handling missing dates, reindexing the DataFrame, etc.\n",
    "<br>\n",
    "<br> 2) Once we turned our imperfect date range into a datetime object (*data.index*), we use it as input in the *difference()* function: The function is applied to a perfect date range between our minimum and maximum dates, and outputs those dates which cannot be found in *data.index*. We print the number of missing rows as well as the actual date rows.\n",
    "<br>\n",
    "<br> 3) We create a perfect date range between our minimum and maximum dates, and use that date range in *reindex()* as our replacement for the current (imperfect) date range. Afterwards we present the same print statements as before, to compare if our correction worked.\n",
    "<br>\n",
    "<br> 4) We reset the index, which means that an integer number is again used as the index. The column *index* is now just a pandas series and we rename it to the original name of the date column, just so that this cell can be re-run repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58c4f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_date_rows(data, date_column_name):\n",
    "    \n",
    "    # Before dropping the date_column_name (see below): Use it to create a column index number and the date range min/max\n",
    "    date_column_index = data.columns.get_loc(date_column_name)\n",
    "    date_range_min = min(data.iloc[:, date_column_index].dt.strftime('%Y-%m-%d'))\n",
    "    date_range_max = max(data.iloc[:, date_column_index].dt.strftime('%Y-%m-%d'))\n",
    "    data = data.set_index(date_column_name, drop = True) # Set date as index; the original date_column_name is dropped here\n",
    "    \n",
    "    # Find missing date rows\n",
    "    data.index = pd.to_datetime(data.index) # format to date time object\n",
    "    date_missing_before = pd.date_range(start = str(date_range_min), end = str(date_range_max)).difference(data.index)\n",
    "    print(str(date_missing_before.size) + \" date rows are missing (before fix)\")\n",
    "    print(date_missing_before) \n",
    "    \n",
    "    # Add the missing date rows\n",
    "    date_range = pd.date_range(str(date_range_min), str(date_range_max)) # creates perfect date range\n",
    "    data.index = pd.DatetimeIndex(data.index)\n",
    "    data = data.reindex(date_range)\n",
    "    date_missing_after = pd.date_range(start = str(date_range_min), end = str(date_range_max)).difference(data.index)\n",
    "    print(str(date_missing_after.size) + \" date rows are missing (after fix)\")\n",
    "    print(date_missing_after)\n",
    "    \n",
    "    data.reset_index(inplace = True, drop = False) # needed to undo the index setting above; inplace = True means the \n",
    "    # original air dataframe is targeted; drop = False retrieves the original date column.\n",
    "    data.rename(columns={'index': date_column_name}, inplace = True) # returning to original column name, so this cell can be re-run easier\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21077f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York air quality data:\n",
      "39 date rows are missing (before fix)\n",
      "DatetimeIndex(['2014-12-29', '2014-12-30', '2014-12-31', '2016-01-01',\n",
      "               '2017-01-28', '2017-09-06', '2017-09-07', '2017-09-08',\n",
      "               '2018-12-31', '2019-10-07', '2019-10-08', '2019-10-09',\n",
      "               '2019-10-10', '2019-10-11', '2019-10-12', '2019-10-13',\n",
      "               '2019-10-14', '2019-10-15', '2019-10-16', '2019-10-17',\n",
      "               '2019-10-18', '2019-10-19', '2019-10-20', '2019-10-21',\n",
      "               '2019-10-22', '2019-10-23', '2019-10-24', '2019-10-25',\n",
      "               '2019-10-26', '2019-10-27', '2019-10-28', '2019-10-29',\n",
      "               '2019-10-30', '2019-10-31', '2019-11-01', '2019-11-02',\n",
      "               '2019-12-30', '2019-12-31', '2020-03-30'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "0 date rows are missing (after fix)\n",
      "DatetimeIndex([], dtype='datetime64[ns]', freq='D')\n"
     ]
    }
   ],
   "source": [
    "# 3.1.5) Apply the add_missing_date_rows() function to air data\n",
    "print(\"New York air quality data:\")\n",
    "ny_air = add_missing_date_rows(data = ny_air, date_column_name = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac46bbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokyo air quality data:\n",
      "15 date rows are missing (before fix)\n",
      "DatetimeIndex(['2014-01-24', '2014-01-25', '2014-12-28', '2014-12-29',\n",
      "               '2014-12-30', '2015-01-10', '2017-09-07', '2017-09-08',\n",
      "               '2017-09-09', '2019-12-30', '2020-03-30', '2020-09-08',\n",
      "               '2020-09-09', '2020-09-10', '2020-09-11'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "0 date rows are missing (after fix)\n",
      "DatetimeIndex([], dtype='datetime64[ns]', freq='D')\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokyo air quality data:\")\n",
    "tk_air = add_missing_date_rows(data = tk_air, date_column_name = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "263e0f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milan air quality data:\n",
      "147 date rows are missing (before fix)\n",
      "DatetimeIndex(['2014-12-29', '2014-12-30', '2015-12-30', '2017-03-10',\n",
      "               '2017-09-05', '2017-09-06', '2017-09-07', '2017-09-08',\n",
      "               '2017-09-09', '2017-09-10',\n",
      "               ...\n",
      "               '2021-01-02', '2021-04-02', '2021-04-03', '2021-04-04',\n",
      "               '2021-05-03', '2021-05-04', '2021-10-14', '2022-07-19',\n",
      "               '2022-07-20', '2022-07-31'],\n",
      "              dtype='datetime64[ns]', length=147, freq=None)\n",
      "0 date rows are missing (after fix)\n",
      "DatetimeIndex([], dtype='datetime64[ns]', freq='D')\n"
     ]
    }
   ],
   "source": [
    "print(\"Milan air quality data:\")\n",
    "ml_air = add_missing_date_rows(data = ml_air, date_column_name = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d32d85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York health data:\n",
      "0 date rows are missing (before fix)\n",
      "DatetimeIndex([], dtype='datetime64[ns]', freq='D')\n",
      "0 date rows are missing (after fix)\n",
      "DatetimeIndex([], dtype='datetime64[ns]', freq='D')\n"
     ]
    }
   ],
   "source": [
    "# 3.1.6) Apply the add_missing_date_rows() function to health data\n",
    "print(\"New York health data:\")\n",
    "ny_health = add_missing_date_rows(data = ny_health, date_column_name = 'date_of_interest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b36a1e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokyo health data:\n",
      "0 date rows are missing (before fix)\n",
      "DatetimeIndex([], dtype='datetime64[ns]', freq='D')\n",
      "0 date rows are missing (after fix)\n",
      "DatetimeIndex([], dtype='datetime64[ns]', freq='D')\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokyo health data:\")\n",
    "tk_health = add_missing_date_rows(data = tk_health, date_column_name = 'death_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d740767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milan health data:\n",
      "0 date rows are missing (before fix)\n",
      "DatetimeIndex([], dtype='datetime64[ns]', freq='D')\n",
      "0 date rows are missing (after fix)\n",
      "DatetimeIndex([], dtype='datetime64[ns]', freq='D')\n"
     ]
    }
   ],
   "source": [
    "print(\"Milan health data:\")\n",
    "ml_health = add_missing_date_rows(data = ml_health, date_column_name = 'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf0aa1",
   "metadata": {},
   "source": [
    "## 3.2) Merge data from air and health into the final dataframe; process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65d6436",
   "metadata": {},
   "source": [
    "We first merge all the air quality tables together, using outer joins and the key *date*. For the health data we do the same, but we have to specify different keys depending on the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5778056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2.1) Outer merge all air data sets by their date columns\n",
    "air = pd.merge(pd.merge(ny_air, tk_air, how = 'outer', on = 'date'), ml_air, how = 'outer', on = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9089f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2.2) Outer merge all health data sets by their date columns - the Tokyo date range includes those from New York and Milan\n",
    "health = pd.merge(pd.merge(tk_health, ny_health, how = 'outer', left_on = 'death_date', right_on = 'date_of_interest'),\\\n",
    "                  ml_health, how = 'outer', left_on = 'death_date', right_on = 'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b916203",
   "metadata": {},
   "source": [
    "In the following, we merge air and health by their date columns (*date* and *death_date*). By creating *df_merged*, we get the columns *date_x* and *date_y*, as *date* is already available in both the air and the health table.\n",
    "<br>\n",
    "We then use the *combine_first()* function to create a single date column which combines the entries from both date columns. Finally, we define the exact date range and 'cut' our dataframe accordingly - creating *df_final*. We replace empty string values by *NaN*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37f8140d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368\n"
     ]
    }
   ],
   "source": [
    "# 3.2.3) Outer merge air and health by their date columns; change all variables of interest to Dtype 'int64'\n",
    "df_merged = pd.merge(air, health, how = 'outer', left_on = 'date', right_on = 'death_date')\n",
    "\n",
    "# 3.2.4) Create date_merged column which combines the entries from both date columns\n",
    "df_merged['date_merged'] = df_merged['date_x']\n",
    "df_merged['date_merged'] = df_merged.date_merged.combine_first(df_merged.death_date)\n",
    "\n",
    "# 3.2.5 Create final dataframe - choose time period\n",
    "set_start_date = '2020-03-03'\n",
    "set_end_date = '2020-06-26'\n",
    "index_start_date = df_merged.loc[df_merged.date_merged == set_start_date].index[0]\n",
    "index_end_date = df_merged.loc[df_merged.date_merged == set_end_date].index[0]\n",
    "print(index_end_date)\n",
    "df_final = df_merged.copy()\n",
    "df_final = df_final.iloc[index_start_date:index_end_date, ]\n",
    "\n",
    "# 3.2.6) Replace empty values by NaN \n",
    "df_final = df_final.replace(r'^\\s*$', np.nan, regex = True) # we set regex = True as we are replacing a string value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9147414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 115 entries, 2253 to 2367\n",
      "Data columns (total 24 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   date_x            115 non-null    datetime64[ns]\n",
      " 1   pm25_ny           113 non-null    object        \n",
      " 2   o3_ny             56 non-null     object        \n",
      " 3   no2_ny            105 non-null    object        \n",
      " 4   co_ny             37 non-null     object        \n",
      " 5   pm25_tk           112 non-null    object        \n",
      " 6   pm10_tk           112 non-null    object        \n",
      " 7   o3_tk             113 non-null    object        \n",
      " 8   no2_tk            113 non-null    object        \n",
      " 9   so2_tk            98 non-null     object        \n",
      " 10  co_tk             110 non-null    object        \n",
      " 11  pm25_ml           111 non-null    object        \n",
      " 12  pm10_ml           111 non-null    object        \n",
      " 13  no2_ml            115 non-null    object        \n",
      " 14  co_ml             0 non-null      float64       \n",
      " 15  death_date        115 non-null    datetime64[ns]\n",
      " 16  death_tk          115 non-null    float64       \n",
      " 17  date_of_interest  115 non-null    datetime64[ns]\n",
      " 18  death_ny          115 non-null    float64       \n",
      " 19  date_y            115 non-null    datetime64[ns]\n",
      " 20  cum_case_ml       115 non-null    object        \n",
      " 21  case_ml           115 non-null    Int64         \n",
      " 22  death_ml          115 non-null    Int64         \n",
      " 23  date_merged       115 non-null    datetime64[ns]\n",
      "dtypes: Int64(2), datetime64[ns](5), float64(3), object(14)\n",
      "memory usage: 22.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# 3.2.7) View the dataframe again  \n",
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c805391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date_x pm25_ny o3_ny no2_ny co_ny pm25_tk pm10_tk o3_tk no2_tk  \\\n",
      "2253 2020-03-03      37   NaN     10   NaN      32      14    22     20   \n",
      "2254 2020-03-04      20   NaN     12   NaN      46       9    33     11   \n",
      "2255 2020-03-05      20   NaN     19   NaN      32      11    34     12   \n",
      "2256 2020-03-06      23   NaN      9   NaN      37      13    32     16   \n",
      "2257 2020-03-07      24   NaN     12   NaN      42      14    30      9   \n",
      "2258 2020-03-08      27   NaN     24   NaN      44      12    26     16   \n",
      "2259 2020-03-09      47   NaN     21   NaN      32      26    20     24   \n",
      "2260 2020-03-10      43   NaN     15   NaN      51      20    34     17   \n",
      "2261 2020-03-11      26   NaN     19   NaN      32      18    32     16   \n",
      "2262 2020-03-12      39   NaN     13   NaN      37      24    34     26   \n",
      "2263 2020-03-13      28   NaN      7   NaN      55      16    30     14   \n",
      "2264 2020-03-14      28   NaN      7   NaN      36      11    32     12   \n",
      "2265 2020-03-15      22   NaN     11   NaN      35       8    32     13   \n",
      "2266 2020-03-16      22   NaN     20   NaN      29      15    32     22   \n",
      "2267 2020-03-17      33   NaN     14   NaN      39      18    33     31   \n",
      "2268 2020-03-18      29   NaN     16   NaN      53      21    29     28   \n",
      "2269 2020-03-19      22   NaN     21   NaN      61       9    34     10   \n",
      "2270 2020-03-20      41   NaN      5   NaN      25      19    32     21   \n",
      "2271 2020-03-21      22   NaN      7   NaN      50      22    48     16   \n",
      "2272 2020-03-22      29   NaN     12   NaN      57      16    37     17   \n",
      "2273 2020-03-23      21   NaN     10   NaN      50       9    35     10   \n",
      "2274 2020-03-24      20   NaN      9   NaN      33      11    33     18   \n",
      "2275 2020-03-25      24   NaN     16   NaN      33      22    22     37   \n",
      "2276 2020-03-26      31   NaN     16   NaN      60      21    31     27   \n",
      "2277 2020-03-27      37   NaN     10   NaN      58      16    32     14   \n",
      "2278 2020-03-28      26   NaN      3   NaN      42       4    28      5   \n",
      "2279 2020-03-29       7   NaN    NaN   NaN      12     NaN   NaN    NaN   \n",
      "2280 2020-03-30     NaN   NaN    NaN   NaN     NaN     NaN   NaN    NaN   \n",
      "2281 2020-03-31     NaN   NaN      8   NaN     NaN      17    27     18   \n",
      "2282 2020-04-01      20   NaN      6   NaN      50       6    41      7   \n",
      "2283 2020-04-02      14   NaN      6   NaN      23      17    30     24   \n",
      "2284 2020-04-03      13   NaN      8   NaN      50      20    44     18   \n",
      "2285 2020-04-04      14   NaN     19   NaN      60      14    35      8   \n",
      "2286 2020-04-05      41   NaN      9   NaN      44      12    38     16   \n",
      "2287 2020-04-06      25   NaN     12   NaN      38      18    40     22   \n",
      "2288 2020-04-07      27   NaN     16   NaN      55      23    49     26   \n",
      "2289 2020-04-08      41   NaN     12   NaN      65      18    46     17   \n",
      "2290 2020-04-09      24   NaN      6   NaN      55       8    37      9   \n",
      "2291 2020-04-10      20   NaN      7   NaN      33      10    39     10   \n",
      "2292 2020-04-11      26   NaN     16   NaN      34      12    38      6   \n",
      "2293 2020-04-12      39   NaN      7   NaN      40       5    36     11   \n",
      "2294 2020-04-13      29   NaN      8   NaN      20       7    35      8   \n",
      "2295 2020-04-14      15    34      7   NaN      21      18    38     22   \n",
      "2296 2020-04-15      26    33      8     1      49      20    37     14   \n",
      "2297 2020-04-16      25    36     12     2      56      18    36     16   \n",
      "2298 2020-04-17      24    31     10     2      47      15    35     14   \n",
      "2299 2020-04-18      22    40     12     2      38       8    37      9   \n",
      "2300 2020-04-19      38    32     13     2      25       8    32     14   \n",
      "2301 2020-04-20      21    30      8     1      22      23    28     31   \n",
      "2302 2020-04-21      20    31    NaN   NaN      54      18    37     14   \n",
      "2303 2020-04-22      13    25    NaN   NaN      43      13    38     15   \n",
      "2304 2020-04-23      22    22     15     2      31      12    38     14   \n",
      "2305 2020-04-24      20    12     17     2      28      10    45     14   \n",
      "2306 2020-04-25      33   NaN      7   NaN      36      20    61      5   \n",
      "2307 2020-04-26      16   NaN      8   NaN      61      16    42     12   \n",
      "2308 2020-04-27       8   NaN     10   NaN      50      15    40     18   \n",
      "2309 2020-04-28      17   NaN     13   NaN      48      19    45     18   \n",
      "2310 2020-04-29      22   NaN      8   NaN      55      28    64     24   \n",
      "2311 2020-04-30      19    33     16     2      71      26    44     13   \n",
      "2312 2020-05-01      11    34      6     1      68      43    43     22   \n",
      "2313 2020-05-02      17    43      9     2     102      31    36     11   \n",
      "2314 2020-05-03      35    36      7     1      81      15    33     13   \n",
      "2315 2020-05-04      21    37      6     1      46      12    45      7   \n",
      "2316 2020-05-05      16    32     11     1      38      12    31      6   \n",
      "2317 2020-05-06      19    37      9     2      29      10    40      8   \n",
      "2318 2020-05-07      18   NaN     11   NaN      19      15    40     15   \n",
      "2319 2020-05-08      17   NaN      5   NaN      44      13    36      8   \n",
      "2320 2020-05-09      11    39      6     1      37      10    33      5   \n",
      "2321 2020-05-10      15    29     10     1      29      16    36     15   \n",
      "2322 2020-05-11      22    37      7     1      40      20    49     17   \n",
      "2323 2020-05-12      14    33     12     1      50      29    53     16   \n",
      "2324 2020-05-13      24    40     16     3      68      18    43     20   \n",
      "2325 2020-05-14      33    46     16     2      48      21    39     24   \n",
      "2326 2020-05-15      51    38      6     1      53      16    32     16   \n",
      "2327 2020-05-16      24    38      3     1      39      17    51     13   \n",
      "2328 2020-05-17       9    38     11   NaN      45      19    30     16   \n",
      "2329 2020-05-18      14    34      8   NaN      51     NaN    31     14   \n",
      "2330 2020-05-19      17    34     10   NaN     NaN      13    21     10   \n",
      "2331 2020-05-20      12    30     10   NaN      28      13    22     12   \n",
      "2332 2020-05-21      23    31     15   NaN      31      12    23     12   \n",
      "2333 2020-05-22      29    31      6   NaN      30      14    34     18   \n",
      "2334 2020-05-23      14    31      4   NaN      32      11    45     12   \n",
      "2335 2020-05-24      24    27      6   NaN      28      15    35     21   \n",
      "2336 2020-05-25      23    23     14   NaN      36      23    27     28   \n",
      "2337 2020-05-26      35   NaN     10   NaN      51      22    53     19   \n",
      "2338 2020-05-27       6   NaN      4   NaN      53      15    48     16   \n",
      "2339 2020-05-28      15   NaN      7   NaN      47      15    43     18   \n",
      "2340 2020-05-29      19   NaN    NaN   NaN      41      19    41     18   \n",
      "2341 2020-05-30      21   NaN    NaN   NaN      49      15    33     14   \n",
      "2342 2020-05-31       9    33      4   NaN      39      19    29     19   \n",
      "2343 2020-06-01      11    42     16   NaN      41      22    26     23   \n",
      "2344 2020-06-02      24    44      9   NaN      47      27    48     23   \n",
      "2345 2020-06-03      44    53     13   NaN      55      45    60     21   \n",
      "2346 2020-06-04      40    38     10   NaN      83      35    45     27   \n",
      "2347 2020-06-05      37    50      8   NaN      68      37    52     16   \n",
      "2348 2020-06-06      26    31      3   NaN      69      22    41      7   \n",
      "2349 2020-06-07      20    34      8     1      51      25    45     20   \n",
      "2350 2020-06-08      18    61     15     2      51      26    33     21   \n",
      "2351 2020-06-09      42    41     10     1      56      13    32     10   \n",
      "2352 2020-06-10      45    26      7     1      31      14    15      8   \n",
      "2353 2020-06-11      35    46     14     2      19      18    19     13   \n",
      "2354 2020-06-12      28    36      5     1      26      15    25     14   \n",
      "2355 2020-06-13      11    30      3     1      22      26    26     15   \n",
      "2356 2020-06-14      17    31     10     1      50      32    44     27   \n",
      "2357 2020-06-15      26    32     14     1      50      29    42     24   \n",
      "2358 2020-06-16      23    30     10     1      55      19    44     16   \n",
      "2359 2020-06-17      20    10      7     1      42      23    35     17   \n",
      "2360 2020-06-18      16   NaN    NaN   NaN      43      13    36     16   \n",
      "2361 2020-06-19      22   NaN    NaN   NaN      27      17    46     20   \n",
      "2362 2020-06-20      42   NaN      9   NaN      35      16    30     10   \n",
      "2363 2020-06-21      29   NaN    NaN   NaN      32      15    26     11   \n",
      "2364 2020-06-22      23   NaN    NaN   NaN      35      17    33     20   \n",
      "2365 2020-06-23      36    37     14     1      30      21    23     15   \n",
      "2366 2020-06-24      47    52      9     1      40      15    17     15   \n",
      "2367 2020-06-25      37    44     25     1      30      37    52     24   \n",
      "\n",
      "     so2_tk  ... co_ml death_date death_tk date_of_interest  death_ny  \\\n",
      "2253      1  ...   NaN 2020-03-03      0.0       2020-03-03       0.0   \n",
      "2254    NaN  ...   NaN 2020-03-04      0.0       2020-03-04       0.0   \n",
      "2255      1  ...   NaN 2020-03-05      0.0       2020-03-05       0.0   \n",
      "2256      1  ...   NaN 2020-03-06      0.0       2020-03-06       0.0   \n",
      "2257    NaN  ...   NaN 2020-03-07      0.0       2020-03-07       0.0   \n",
      "2258      1  ...   NaN 2020-03-08      0.0       2020-03-08       0.0   \n",
      "2259      1  ...   NaN 2020-03-09      1.0       2020-03-09       0.0   \n",
      "2260      1  ...   NaN 2020-03-10      0.0       2020-03-10       0.0   \n",
      "2261      1  ...   NaN 2020-03-11      0.0       2020-03-11       1.0   \n",
      "2262      1  ...   NaN 2020-03-12      1.0       2020-03-12       0.0   \n",
      "2263    NaN  ...   NaN 2020-03-13      0.0       2020-03-13       0.0   \n",
      "2264      1  ...   NaN 2020-03-14      0.0       2020-03-14       2.0   \n",
      "2265      1  ...   NaN 2020-03-15      0.0       2020-03-15       4.0   \n",
      "2266      1  ...   NaN 2020-03-16      0.0       2020-03-16       7.0   \n",
      "2267      1  ...   NaN 2020-03-17      0.0       2020-03-17       8.0   \n",
      "2268      1  ...   NaN 2020-03-18      0.0       2020-03-18      20.0   \n",
      "2269      1  ...   NaN 2020-03-19      1.0       2020-03-19      24.0   \n",
      "2270      1  ...   NaN 2020-03-20      0.0       2020-03-20      47.0   \n",
      "2271      1  ...   NaN 2020-03-21      1.0       2020-03-21      44.0   \n",
      "2272      1  ...   NaN 2020-03-22      0.0       2020-03-22      49.0   \n",
      "2273      1  ...   NaN 2020-03-23      0.0       2020-03-23      82.0   \n",
      "2274      1  ...   NaN 2020-03-24      1.0       2020-03-24      95.0   \n",
      "2275      3  ...   NaN 2020-03-25      0.0       2020-03-25     125.0   \n",
      "2276      1  ...   NaN 2020-03-26      0.0       2020-03-26     186.0   \n",
      "2277    NaN  ...   NaN 2020-03-27      2.0       2020-03-27     216.0   \n",
      "2278    NaN  ...   NaN 2020-03-28      2.0       2020-03-28     272.0   \n",
      "2279    NaN  ...   NaN 2020-03-29      5.0       2020-03-29     290.0   \n",
      "2280    NaN  ...   NaN 2020-03-30      2.0       2020-03-30     323.0   \n",
      "2281    NaN  ...   NaN 2020-03-31      1.0       2020-03-31     388.0   \n",
      "2282      1  ...   NaN 2020-04-01      2.0       2020-04-01     447.0   \n",
      "2283      1  ...   NaN 2020-04-02      2.0       2020-04-02     499.0   \n",
      "2284      1  ...   NaN 2020-04-03      4.0       2020-04-03     499.0   \n",
      "2285    NaN  ...   NaN 2020-04-04      6.0       2020-04-04     511.0   \n",
      "2286      1  ...   NaN 2020-04-05      2.0       2020-04-05     578.0   \n",
      "2287      2  ...   NaN 2020-04-06      1.0       2020-04-06     576.0   \n",
      "2288      1  ...   NaN 2020-04-07      3.0       2020-04-07     598.0   \n",
      "2289      1  ...   NaN 2020-04-08      4.0       2020-04-08     568.0   \n",
      "2290    NaN  ...   NaN 2020-04-09      5.0       2020-04-09     558.0   \n",
      "2291      1  ...   NaN 2020-04-10      5.0       2020-04-10     537.0   \n",
      "2292      1  ...   NaN 2020-04-11      7.0       2020-04-11     544.0   \n",
      "2293    NaN  ...   NaN 2020-04-12     11.0       2020-04-12     573.0   \n",
      "2294      1  ...   NaN 2020-04-13      3.0       2020-04-13     568.0   \n",
      "2295      2  ...   NaN 2020-04-14      4.0       2020-04-14     508.0   \n",
      "2296      1  ...   NaN 2020-04-15      7.0       2020-04-15     456.0   \n",
      "2297      2  ...   NaN 2020-04-16     11.0       2020-04-16     413.0   \n",
      "2298      1  ...   NaN 2020-04-17      7.0       2020-04-17     384.0   \n",
      "2299      1  ...   NaN 2020-04-18      7.0       2020-04-18     390.0   \n",
      "2300      1  ...   NaN 2020-04-19      8.0       2020-04-19     398.0   \n",
      "2301      1  ...   NaN 2020-04-20      9.0       2020-04-20     364.0   \n",
      "2302      1  ...   NaN 2020-04-21      6.0       2020-04-21     323.0   \n",
      "2303      1  ...   NaN 2020-04-22      5.0       2020-04-22     311.0   \n",
      "2304      1  ...   NaN 2020-04-23     12.0       2020-04-23     330.0   \n",
      "2305      1  ...   NaN 2020-04-24      6.0       2020-04-24     314.0   \n",
      "2306      1  ...   NaN 2020-04-25      6.0       2020-04-25     265.0   \n",
      "2307    NaN  ...   NaN 2020-04-26      8.0       2020-04-26     245.0   \n",
      "2308      1  ...   NaN 2020-04-27      5.0       2020-04-27     270.0   \n",
      "2309      2  ...   NaN 2020-04-28     10.0       2020-04-28     229.0   \n",
      "2310      1  ...   NaN 2020-04-29      7.0       2020-04-29     237.0   \n",
      "2311      1  ...   NaN 2020-04-30      8.0       2020-04-30     223.0   \n",
      "2312      4  ...   NaN 2020-05-01      6.0       2020-05-01     209.0   \n",
      "2313      1  ...   NaN 2020-05-02      9.0       2020-05-02     189.0   \n",
      "2314      1  ...   NaN 2020-05-03      8.0       2020-05-03     177.0   \n",
      "2315      1  ...   NaN 2020-05-04      9.0       2020-05-04     158.0   \n",
      "2316    NaN  ...   NaN 2020-05-05      6.0       2020-05-05     151.0   \n",
      "2317    NaN  ...   NaN 2020-05-06      4.0       2020-05-06     151.0   \n",
      "2318      1  ...   NaN 2020-05-07      8.0       2020-05-07     141.0   \n",
      "2319      1  ...   NaN 2020-05-08      9.0       2020-05-08     130.0   \n",
      "2320      1  ...   NaN 2020-05-09      2.0       2020-05-09      99.0   \n",
      "2321      1  ...   NaN 2020-05-10      3.0       2020-05-10     102.0   \n",
      "2322      1  ...   NaN 2020-05-11      2.0       2020-05-11     103.0   \n",
      "2323      2  ...   NaN 2020-05-12      8.0       2020-05-12      91.0   \n",
      "2324      2  ...   NaN 2020-05-13      4.0       2020-05-13      83.0   \n",
      "2325      1  ...   NaN 2020-05-14      1.0       2020-05-14      70.0   \n",
      "2326      1  ...   NaN 2020-05-15      3.0       2020-05-15      86.0   \n",
      "2327      2  ...   NaN 2020-05-16      4.0       2020-05-16      82.0   \n",
      "2328      1  ...   NaN 2020-05-17      3.0       2020-05-17      76.0   \n",
      "2329    NaN  ...   NaN 2020-05-18      4.0       2020-05-18      59.0   \n",
      "2330      1  ...   NaN 2020-05-19      2.0       2020-05-19      55.0   \n",
      "2331    NaN  ...   NaN 2020-05-20      0.0       2020-05-20      79.0   \n",
      "2332      1  ...   NaN 2020-05-21      1.0       2020-05-21      44.0   \n",
      "2333      1  ...   NaN 2020-05-22      3.0       2020-05-22      65.0   \n",
      "2334      1  ...   NaN 2020-05-23      0.0       2020-05-23      60.0   \n",
      "2335      1  ...   NaN 2020-05-24      0.0       2020-05-24      48.0   \n",
      "2336      1  ...   NaN 2020-05-25      2.0       2020-05-25      47.0   \n",
      "2337      1  ...   NaN 2020-05-26      3.0       2020-05-26      36.0   \n",
      "2338      1  ...   NaN 2020-05-27      2.0       2020-05-27      48.0   \n",
      "2339      1  ...   NaN 2020-05-28      0.0       2020-05-28      37.0   \n",
      "2340      1  ...   NaN 2020-05-29      0.0       2020-05-29      43.0   \n",
      "2341      1  ...   NaN 2020-05-30      1.0       2020-05-30      52.0   \n",
      "2342      1  ...   NaN 2020-05-31      0.0       2020-05-31      39.0   \n",
      "2343      1  ...   NaN 2020-06-01      1.0       2020-06-01      33.0   \n",
      "2344      1  ...   NaN 2020-06-02      0.0       2020-06-02      37.0   \n",
      "2345      3  ...   NaN 2020-06-03      4.0       2020-06-03      42.0   \n",
      "2346      3  ...   NaN 2020-06-04      0.0       2020-06-04      33.0   \n",
      "2347      1  ...   NaN 2020-06-05      0.0       2020-06-05      27.0   \n",
      "2348      1  ...   NaN 2020-06-06      0.0       2020-06-06      34.0   \n",
      "2349      1  ...   NaN 2020-06-07      0.0       2020-06-07      24.0   \n",
      "2350      1  ...   NaN 2020-06-08      0.0       2020-06-08      38.0   \n",
      "2351      1  ...   NaN 2020-06-09      4.0       2020-06-09      30.0   \n",
      "2352      1  ...   NaN 2020-06-10      0.0       2020-06-10      32.0   \n",
      "2353      1  ...   NaN 2020-06-11      1.0       2020-06-11      21.0   \n",
      "2354      1  ...   NaN 2020-06-12      0.0       2020-06-12      22.0   \n",
      "2355      1  ...   NaN 2020-06-13      0.0       2020-06-13      15.0   \n",
      "2356      1  ...   NaN 2020-06-14      0.0       2020-06-14      22.0   \n",
      "2357      1  ...   NaN 2020-06-15      2.0       2020-06-15      19.0   \n",
      "2358      1  ...   NaN 2020-06-16      0.0       2020-06-16      16.0   \n",
      "2359      1  ...   NaN 2020-06-17      3.0       2020-06-17      20.0   \n",
      "2360      1  ...   NaN 2020-06-18      1.0       2020-06-18      15.0   \n",
      "2361      1  ...   NaN 2020-06-19      0.0       2020-06-19      15.0   \n",
      "2362      1  ...   NaN 2020-06-20      1.0       2020-06-20      16.0   \n",
      "2363      1  ...   NaN 2020-06-21      2.0       2020-06-21      22.0   \n",
      "2364      1  ...   NaN 2020-06-22      0.0       2020-06-22      18.0   \n",
      "2365    NaN  ...   NaN 2020-06-23      0.0       2020-06-23      17.0   \n",
      "2366      1  ...   NaN 2020-06-24      0.0       2020-06-24      18.0   \n",
      "2367      1  ...   NaN 2020-06-25      0.0       2020-06-25      15.0   \n",
      "\n",
      "         date_y  cum_case_ml case_ml  death_ml date_merged  \n",
      "2253 2020-03-03           93      35         6  2020-03-03  \n",
      "2254 2020-03-04          145      52         9  2020-03-04  \n",
      "2255 2020-03-05          197      52         9  2020-03-05  \n",
      "2256 2020-03-06          267      70        12  2020-03-06  \n",
      "2257 2020-03-07          361      94        16  2020-03-07  \n",
      "2258 2020-03-08          406      45         8  2020-03-08  \n",
      "2259 2020-03-09          506     100        17  2020-03-09  \n",
      "2260 2020-03-10          592      86        15  2020-03-10  \n",
      "2261 2020-03-11          925     333        58  2020-03-11  \n",
      "2262 2020-03-12         1146     221        38  2020-03-12  \n",
      "2263 2020-03-13         1307     161        28  2020-03-13  \n",
      "2264 2020-03-14         1551     244        42  2020-03-14  \n",
      "2265 2020-03-15         1750     199        34  2020-03-15  \n",
      "2266 2020-03-16         1983     233        40  2020-03-16  \n",
      "2267 2020-03-17         2326     343        59  2020-03-17  \n",
      "2268 2020-03-18         2644     318        55  2020-03-18  \n",
      "2269 2020-03-19         3278     634       110  2020-03-19  \n",
      "2270 2020-03-20         3804     526        91  2020-03-20  \n",
      "2271 2020-03-21         4672     868       150  2020-03-21  \n",
      "2272 2020-03-22         5096     424        73  2020-03-22  \n",
      "2273 2020-03-23         5326     230        40  2020-03-23  \n",
      "2274 2020-03-24         5701     375        65  2020-03-24  \n",
      "2275 2020-03-25         6074     373        65  2020-03-25  \n",
      "2276 2020-03-26         6922     848       147  2020-03-26  \n",
      "2277 2020-03-27         7469     547        95  2020-03-27  \n",
      "2278 2020-03-28         7783     314        54  2020-03-28  \n",
      "2279 2020-03-29         8329     546        94  2020-03-29  \n",
      "2280 2020-03-30         8676     347        60  2020-03-30  \n",
      "2281 2020-03-31         8911     235        41  2020-03-31  \n",
      "2282 2020-04-01         9522     611       106  2020-04-01  \n",
      "2283 2020-04-02        10004     482        83  2020-04-02  \n",
      "2284 2020-04-03        10391     387        67  2020-04-03  \n",
      "2285 2020-04-04        10819     428        74  2020-04-04  \n",
      "2286 2020-04-05        11230     411        71  2020-04-05  \n",
      "2287 2020-04-06        11538     308        53  2020-04-06  \n",
      "2288 2020-04-07        11787     249        43  2020-04-07  \n",
      "2289 2020-04-08        12039     252        44  2020-04-08  \n",
      "2290 2020-04-09        12479     440        76  2020-04-09  \n",
      "2291 2020-04-10        12748     269        47  2020-04-10  \n",
      "2292 2020-04-11        13268     520        90  2020-04-11  \n",
      "2293 2020-04-12        13680     412        71  2020-04-12  \n",
      "2294 2020-04-13        14161     481        83  2020-04-13  \n",
      "2295 2020-04-14        14350     189        33  2020-04-14  \n",
      "2296 2020-04-15        14675     325        56  2020-04-15  \n",
      "2297 2020-04-16        14952     277        48  2020-04-16  \n",
      "2298 2020-04-17        15277     325        56  2020-04-17  \n",
      "2299 2020-04-18        15546     269        47  2020-04-18  \n",
      "2300 2020-04-19        15825     279        48  2020-04-19  \n",
      "2301 2020-04-20        16112     287        50  2020-04-20  \n",
      "2302 2020-04-21        16520     408        71  2020-04-21  \n",
      "2303 2020-04-22        17000     480        83  2020-04-22  \n",
      "2304 2020-04-23        17277     277        48  2020-04-23  \n",
      "2305 2020-04-24        17689     412        71  2020-04-24  \n",
      "2306 2020-04-25        17908     219        38  2020-04-25  \n",
      "2307 2020-04-26        18371     463        80  2020-04-26  \n",
      "2308 2020-04-27        18559     188        33  2020-04-27  \n",
      "2309 2020-04-28        18837     278        48  2020-04-28  \n",
      "2310 2020-04-29        19121     284        49  2020-04-29  \n",
      "2311 2020-04-30        19337     216        37  2020-04-30  \n",
      "2312 2020-05-01        19701     364        63  2020-05-01  \n",
      "2313 2020-05-02        19950     249        43  2020-05-02  \n",
      "2314 2020-05-03        20068     118        20  2020-05-03  \n",
      "2315 2020-05-04        20254     186        32  2020-05-04  \n",
      "2316 2020-05-05        20398     144        25  2020-05-05  \n",
      "2317 2020-05-06        20711     313        54  2020-05-06  \n",
      "2318 2020-05-07        20893     182        31  2020-05-07  \n",
      "2319 2020-05-08        21094     201        35  2020-05-08  \n",
      "2320 2020-05-09        21272     178        31  2020-05-09  \n",
      "2321 2020-05-10        21376     104        18  2020-05-10  \n",
      "2322 2020-05-11        21490     114        20  2020-05-11  \n",
      "2323 2020-05-12        21626     136        24  2020-05-12  \n",
      "2324 2020-05-13        21731     105        18  2020-05-13  \n",
      "2325 2020-05-14        21900     169        29  2020-05-14  \n",
      "2326 2020-05-15        21966      66        11  2020-05-15  \n",
      "2327 2020-05-16        22041      75        13  2020-05-16  \n",
      "2328 2020-05-17        22151     110        19  2020-05-17  \n",
      "2329 2020-05-18        22222      71        12  2020-05-18  \n",
      "2330 2020-05-19        22324     102        18  2020-05-19  \n",
      "2331 2020-05-20        22372      48         8  2020-05-20  \n",
      "2332 2020-05-21        22455      83        14  2020-05-21  \n",
      "2333 2020-05-22        22528      73        13  2020-05-22  \n",
      "2334 2020-05-23        22616      88        15  2020-05-23  \n",
      "2335 2020-05-24        22680      64        11  2020-05-24  \n",
      "2336 2020-05-25        22726      46         8  2020-05-25  \n",
      "2337 2020-05-26        22764      38         7  2020-05-26  \n",
      "2338 2020-05-27        22832      68        12  2020-05-27  \n",
      "2339 2020-05-28        22908      76        13  2020-05-28  \n",
      "2340 2020-05-29        22982      74        13  2020-05-29  \n",
      "2341 2020-05-30        23044      62        11  2020-05-30  \n",
      "2342 2020-05-31        23076      32         6  2020-05-31  \n",
      "2343 2020-06-01        23094      18         3  2020-06-01  \n",
      "2344 2020-06-02        23139      45         8  2020-06-02  \n",
      "2345 2020-06-03        23176      37         6  2020-06-03  \n",
      "2346 2020-06-04        23207      31         5  2020-06-04  \n",
      "2347 2020-06-05        23306      99        17  2020-06-05  \n",
      "2348 2020-06-06        23365      59        10  2020-06-06  \n",
      "2349 2020-06-07        23408      43         7  2020-06-07  \n",
      "2350 2020-06-08        23437      29         5  2020-06-08  \n",
      "2351 2020-06-09        23483      46         8  2020-06-09  \n",
      "2352 2020-06-10        23510      27         5  2020-06-10  \n",
      "2353 2020-06-11        23581      71        12  2020-06-11  \n",
      "2354 2020-06-12        23669      88        15  2020-06-12  \n",
      "2355 2020-06-13        23766      97        17  2020-06-13  \n",
      "2356 2020-06-14        23811      45         8  2020-06-14  \n",
      "2357 2020-06-15        23863      52         9  2020-06-15  \n",
      "2358 2020-06-16        23905      42         7  2020-06-16  \n",
      "2359 2020-06-17        23966      61        11  2020-06-17  \n",
      "2360 2020-06-18        24018      52         9  2020-06-18  \n",
      "2361 2020-06-19        24061      43         7  2020-06-19  \n",
      "2362 2020-06-20        24130      69        12  2020-06-20  \n",
      "2363 2020-06-21        24161      31         5  2020-06-21  \n",
      "2364 2020-06-22        24184      23         4  2020-06-22  \n",
      "2365 2020-06-23        24210      26         4  2020-06-23  \n",
      "2366 2020-06-24        24239      29         5  2020-06-24  \n",
      "2367 2020-06-25        24267      28         5  2020-06-25  \n",
      "\n",
      "[115 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', 500): # change number of rows if needed\n",
    "   print(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb395ad8",
   "metadata": {},
   "source": [
    "We can see that co_ml, co_ny and o3_ny are showing a lot of missing values. We keep o3_ny as it contains around 50% of \n",
    "the rows, but co_ml and co_ny will be excluded from here onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaadb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2.8) Exclude co_ml and co_ny from the final dataframe\n",
    "df_final = df_final.loc[:, ['date_merged', 'pm25_ny', 'o3_ny', 'no2_ny', 'pm25_tk', 'pm10_tk', 'o3_tk', 'no2_tk', 'so2_tk',\\\n",
    "                           'co_tk', 'pm25_ml', 'pm10_ml', 'no2_ml', 'death_ny', 'death_tk', 'death_ml']]\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e08dcdb",
   "metadata": {},
   "source": [
    "## 3.3) Process final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d382a712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3.3.1) Rename columns\n",
    "df_final.rename(columns = {'date_merged': 'date'}, inplace = True)\n",
    "\n",
    "# 3.3.2) Replace empty values by NaN\n",
    "df_final = df_final.replace(r'^\\s*$', np.nan, regex = True) \n",
    "\n",
    "# 3.3.3) Change all variables (except for the date column) to Dtype 'Int64Dtype' - it allows better handling of NaN values\n",
    "df_final = df_final.astype({'pm25_ny': pd.Int64Dtype(), 'o3_ny': pd.Int64Dtype(), 'no2_ny': pd.Int64Dtype(),\\\n",
    "                        'pm25_tk': pd.Int64Dtype(), 'pm10_tk': pd.Int64Dtype(), 'o3_tk': pd.Int64Dtype(), 'no2_tk': pd.Int64Dtype(), 'so2_tk': pd.Int64Dtype(), 'co_tk': pd.Int64Dtype(),\\\n",
    "                        'pm25_ml': pd.Int64Dtype(), 'pm10_ml': pd.Int64Dtype(), 'no2_ml': pd.Int64Dtype(),\\\n",
    "                        'death_ny': pd.Int64Dtype(), 'death_tk': pd.Int64Dtype(), 'death_ml': pd.Int64Dtype()})\n",
    "\n",
    "# 3.3.4) Create interpolated versions of our raw variables (linear interpolation to replace NaN values)\n",
    "# Note: o3_ny cannot be interpolated as it starts with NaN values\n",
    "df_final[['pm25_ny_ip', 'no2_ny_ip',\\\n",
    "          'pm25_tk_ip', 'pm10_tk_ip', 'o3_tk_ip', 'no2_tk_ip', 'so2_tk_ip', 'co_tk_ip',\\\n",
    "          'pm25_ml_ip', 'pm10_ml_ip', 'no2_ml_ip',\\\n",
    "          'death_ny_ip', 'death_tk_ip', 'death_ml_ip']] = df_final[['pm25_ny', 'no2_ny',\\\n",
    "                                        'pm25_tk', 'pm10_tk', 'o3_tk', 'no2_tk', 'so2_tk', 'co_tk',\\\n",
    "                                        'pm25_ml', 'pm10_ml', 'no2_ml',\\\n",
    "                                        'death_ny', 'death_tk', 'death_ml']].astype(float).interpolate(method = 'linear').round(0).astype(pd.Int64Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e5db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3.5) Create additional log and difference variables - based on both raw and interpolated data\n",
    "\n",
    "# 3.3.5.1) For the raw variables (with NAs)\n",
    "df_final[['ln_pm25_ny', 'ln_o3_ny', 'ln_no2_ny',\\\n",
    "            'ln_pm25_tk', 'ln_pm10_tk', 'ln_o3_tk', 'ln_no2_tk', 'ln_so2_tk', 'ln_co_tk',\\\n",
    "            'ln_pm25_ml', 'ln_pm10_ml', 'ln_no2_ml',\\\n",
    "            'ln_death_ny', 'ln_death_tk', 'ln_death_ml']] = np.log(df_final[['pm25_ny', 'o3_ny', 'no2_ny',\\\n",
    "                                                'pm25_tk', 'pm10_tk', 'o3_tk', 'no2_tk', 'so2_tk', 'co_tk',\\\n",
    "                                                'pm25_ml', 'pm10_ml', 'no2_ml',\\\n",
    "                                                'death_ny', 'death_tk', 'death_ml']].astype(float)) # warning: log(0) = -Inf\n",
    "df_final[['d_pm25_ny', 'd_o3_ny', 'd_no2_ny',\\\n",
    "            'd_pm25_tk', 'd_pm10_tk', 'd_o3_tk', 'd_no2_tk', 'd_so2_tk', 'd_co_tk',\\\n",
    "            'd_pm25_ml', 'd_pm10_ml', 'd_no2_ml',\\\n",
    "            'd_death_ny', 'd_death_tk', 'd_death_ml']] = df_final[['pm25_ny', 'o3_ny', 'no2_ny',\\\n",
    "                                                'pm25_tk', 'pm10_tk', 'o3_tk', 'no2_tk', 'so2_tk', 'co_tk',\\\n",
    "                                                'pm25_ml', 'pm10_ml', 'no2_ml',\\\n",
    "                                                'death_ny', 'death_tk', 'death_ml']].astype(pd.Int64Dtype()).diff(periods = 1, axis = 0)\n",
    "\n",
    "# 3.3.5.2) For all available interpolated variables; \n",
    "# Note: Interpolation is not available for o3_ny - see 3.3.4)\n",
    "df_final[['ln_pm25_ny_ip', 'ln_no2_ny_ip',\\\n",
    "            'ln_pm25_tk_ip', 'ln_pm10_tk_ip', 'ln_o3_tk_ip', 'ln_no2_tk_ip', 'ln_so2_tk_ip', 'ln_co_tk_ip',\\\n",
    "            'ln_pm25_ml_ip', 'ln_pm10_ml_ip', 'ln_no2_ml_ip',\\\n",
    "            'ln_death_ny_ip', 'ln_death_tk_ip', 'ln_death_ml_ip']] = np.log(df_final[['pm25_ny_ip', 'no2_ny_ip',\\\n",
    "                                                'pm25_tk_ip', 'pm10_tk_ip', 'o3_tk_ip', 'no2_tk_ip', 'so2_tk_ip', 'co_tk_ip',\\\n",
    "                                                'pm25_ml_ip', 'pm10_ml_ip', 'no2_ml_ip',\\\n",
    "                                                'death_ny_ip', 'death_tk_ip', 'death_ml_ip']]).astype(float) # warning: log(0) = -Inf\n",
    "df_final[['d_pm25_ny_ip', 'd_no2_ny_ip',\\\n",
    "            'd_pm25_tk_ip', 'd_pm10_tk_ip', 'd_o3_tk_ip', 'd_no2_tk_ip', 'd_so2_tk_ip', 'd_co_tk_ip',\\\n",
    "            'd_pm25_ml_ip', 'd_pm10_ml_ip', 'd_no2_ml_ip',\\\n",
    "            'd_death_ny_ip', 'd_death_tk_ip', 'd_death_ml_ip']] = df_final[['pm25_ny_ip', 'no2_ny_ip',\\\n",
    "                                            'pm25_tk_ip', 'pm10_tk_ip', 'o3_tk_ip', 'no2_tk_ip', 'so2_tk_ip', 'co_tk_ip',\\\n",
    "                                            'pm25_ml_ip', 'pm10_ml_ip', 'no2_ml_ip',\\\n",
    "                                            'death_ny_ip', 'death_tk_ip', 'death_ml_ip']].astype(pd.Int64Dtype()).diff(periods = 1, axis = 0)\n",
    "\n",
    "# 3.3.5.3) Replace Inf and -Inf by NaN\n",
    "df_final.replace([np.inf, -np.inf], np.nan, inplace = True) # replace Inf and -Inf by NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd07cb",
   "metadata": {},
   "source": [
    "## 3.4) View the whole final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac461f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e75f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns for the remainder of the script\n",
    "pd.set_option('display.max_columns', 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa9397",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 10): # change number of rows if needed\n",
    "   print(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df463e",
   "metadata": {},
   "source": [
    "# 4) Descriptive plots\n",
    "## 4.1) Scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cbfcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.1) Create a function for scatterplots (with regression line) that allows us to control the range of the x- and y-variable\n",
    "\n",
    "def scatterplot(df, x_column, y_column, xlim_min = None, xlim_max = None, ylim_min = 1, ylim_max = 590):\n",
    "        \n",
    "    # Create temporary dataframe that removes NAs\n",
    "    df_temp = df.loc[:, [x_column, y_column]].dropna()\n",
    "    \n",
    "    # Define xlim_min and xlim_max in case they are not provided\n",
    "    if xlim_min == None:\n",
    "        xlim_min = min(df_temp.loc[:, x_column])\n",
    "    if xlim_max == None:\n",
    "        xlim_max = max(df_temp.loc[:, x_column])\n",
    "    \n",
    "    # Exclude values outside of the limits - this is not needed for the plot, but for the linear regression\n",
    "    df_temp = df_temp[(df_temp[x_column] >= xlim_min) & (df_temp[x_column] <= xlim_max)\\\n",
    "                      & (df_temp[y_column] >= ylim_min) & (df_temp[y_column] <= ylim_max)]\n",
    "    \n",
    "    # Define x and y indices\n",
    "    x_index = df_temp.columns.get_loc(x_column)\n",
    "    y_index = df_temp.columns.get_loc(y_column)\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plot = df_temp.plot.scatter(x =  x_column, y =  y_column)\n",
    "   \n",
    "    # Create linear regression\n",
    "    X = df_temp.iloc[:, x_index].values.reshape(-1, 1)\n",
    "    Y = df_temp.iloc[:, y_index].values.reshape(-1, 1)\n",
    "    linear_regressor = LinearRegression()\n",
    "    linear_regressor.fit(X, Y)  # perform linear regression\n",
    "    Y_pred = linear_regressor.predict(X)  # make predictions\n",
    "    plt.plot(X, Y_pred, color = 'red')\n",
    "     \n",
    "    # Add title and axis names\n",
    "    plt.title(f\"Scatterplot:\\n {x_column}  vs  {y_column}\")\n",
    "    plt.xlabel(x_column)\n",
    "    plt.ylabel(y_column)\n",
    "    plt.ylim(ylim_min, ylim_max)\n",
    "    plt.xlim(xlim_min, xlim_max)\n",
    "    \n",
    "    # Save the plot into the \\figures directory\n",
    "    plt.savefig(f\"..\\\\figures\\_411_scatter {x_column} vs {y_column}.jpg\", dpi = 300, bbox_inches = 'tight')\n",
    "    \n",
    "    return(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.2) The linear regression in the scatterplot function above shows the following warning when applied to our data:\n",
    "\n",
    "# 'Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24\n",
    "# and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead.'\n",
    "\n",
    "# For a better presentation of the plots we are setting off these warnings:\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67111f99",
   "metadata": {},
   "source": [
    "### 4.1.3) New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36be9d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4.1.3.1) Deaths vs PM2.5\n",
    "scatterplot(df_final, 'pm25_ny', 'death_ny', xlim_min = 20, xlim_max = 78)\n",
    "# scatterplot(df_final, 'pm25_ny', 'death_ny')\n",
    "# scatterplot(df_final, 'pm25_ny_ip', 'death_ny_ip', xlim_min = 20, xlim_max = 78)\n",
    "# scatterplot(df_final, 'pm25_ny_ip', 'death_ny_ip')\n",
    "\n",
    "# 4.1.3.2) Deaths vs NO2\n",
    "scatterplot(df_final, 'no2_ny', 'death_ny', xlim_min = 8, xlim_max = 43)\n",
    "# scatterplot(df_final, 'no2_ny', 'death_ny')\n",
    "# scatterplot(df_final, 'no2_ny_ip', 'death_ny_ip', xlim_min = 8, xlim_max = 43)\n",
    "# scatterplot(df_final, 'no2_ny_ip', 'death_ny_ip')\n",
    "\n",
    "# 4.1.3.3) Deaths vs lnPM2.5\n",
    "scatterplot(df_final, 'ln_pm25_ny', 'death_ny', xlim_min = 3, xlim_max = 4.36)\n",
    "# scatterplot(df_final, 'ln_pm25_ny', 'death_ny')\n",
    "# scatterplot(df_final, 'ln_pm25_ny_ip', 'death_ny_ip', xlim_min = 3, xlim_max = 4.36)\n",
    "# scatterplot(df_final, 'ln_pm25_ny_ip', 'death_ny_ip')\n",
    "\n",
    "# 4.1.3.4) Deaths vs lnNO2\n",
    "scatterplot(df_final, 'ln_no2_ny', 'death_ny', xlim_min = 2.08, xlim_max = 3.76)\n",
    "# scatterplot(df_final, 'ln_no2_ny', 'death_ny')\n",
    "# scatterplot(df_final, 'ln_no2_ny_ip', 'death_ny_ip', xlim_min = 2.08, xlim_max = 3.76)\n",
    "# scatterplot(df_final, 'ln_no2_ny_ip', 'death_ny_ip')\n",
    "\n",
    "# 4.1.3.5) Deaths vs dPM2.5\n",
    "scatterplot(df_final, 'd_pm25_ny', 'death_ny', xlim_min = -1, xlim_max = 0.78)\n",
    "# scatterplot(df_final, 'd_pm25_ny', 'death_ny')\n",
    "# scatterplot(df_final, 'd_pm25_ny_ip', 'death_ny_ip', xlim_min = -1, xlim_max = 0.78)\n",
    "# scatterplot(df_final, 'd_pm25_ny_ip', 'death_ny_ip')\n",
    "\n",
    "# 4.1.3.6) Deaths vs dNO2\n",
    "scatterplot(df_final, 'd_no2_ny', 'death_ny', xlim_min = -1, xlim_max = 1.75)\n",
    "# scatterplot(df_final, 'd_no2_ny', 'death_ny')\n",
    "# scatterplot(df_final, 'd_no2_ny_ip', 'death_ny_ip', xlim_min = -1, xlim_max = 1.75)\n",
    "# scatterplot(df_final, 'd_no2_ny_ip', 'death_ny_ip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85032f9",
   "metadata": {},
   "source": [
    "### 4.1.4) Milan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aeddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.4.1) Deaths vs PM2.5\n",
    "# scatterplot(df_final, 'pm25_ml', 'death_ml', xlim_min = 20, xlim_max = 78)\n",
    "scatterplot(df_final, 'pm25_ml', 'death_ml', ylim_min = 0, ylim_max = 200)\n",
    "# scatterplot(df_final, 'pm25_ml_ip', 'death_ml_ip', xlim_min = 20, xlim_max = 78)\n",
    "# scatterplot(df_final, 'pm25_ml_ip', 'death_ml_ip', ylim_min = 0, ylim_max = 200)\n",
    "\n",
    "# 4.1.4.2) Deaths vs NO2\n",
    "# scatterplot(df_final, 'no2_ml', 'death_ml', xlim_min = 8, xlim_max = 43)\n",
    "scatterplot(df_final, 'no2_ml', 'death_ml', ylim_min = 0, ylim_max = 200)\n",
    "# scatterplot(df_final, 'no2_ml_ip', 'death_ml_ip', xlim_min = 8, xlim_max = 43)\n",
    "# scatterplot(df_final, 'no2_ml_ip', 'death_ml_ip', ylim_min = 0, ylim_max = 200)\n",
    "\n",
    "# 4.1.4.3) Deaths vs lnPM2.5\n",
    "# scatterplot(df_final, 'ln_pm25_ml', 'death_ml', xlim_min = 3, xlim_max = 4.36)\n",
    "scatterplot(df_final, 'ln_pm25_ml', 'death_ml', ylim_min = 0, ylim_max = 200)\n",
    "# scatterplot(df_final, 'ln_pm25_ml_ip', 'death_ml_ip', xlim_min = 3, xlim_max = 4.36)\n",
    "# scatterplot(df_final, 'ln_pm25_ml_ip', 'death_ml_ip', ylim_min = 0, ylim_max = 200)\n",
    "\n",
    "# 4.1.4.4) Deaths vs lnNO2\n",
    "# scatterplot(df_final, 'ln_no2_ml', 'death_ml', xlim_min = 2.08, xlim_max = 3.76)\n",
    "scatterplot(df_final, 'ln_no2_ml', 'death_ml', ylim_min = 0, ylim_max = 200)\n",
    "# scatterplot(df_final, 'ln_no2_ml_ip', 'death_ml_ip', xlim_min = 2.08, xlim_max = 3.76)\n",
    "# scatterplot(df_final, 'ln_no2_ml_ip', 'death_ml_ip', ylim_min = 0, ylim_max = 200)\n",
    "\n",
    "# 4.1.4.5) Deaths vs dPM2.5\n",
    "# scatterplot(df_final, 'd_pm25_ml', 'death_ml', xlim_min = -1, xlim_max = 0.78)\n",
    "scatterplot(df_final, 'd_pm25_ml', 'death_ml', ylim_min = 0, ylim_max = 200)\n",
    "# scatterplot(df_final, 'd_pm25_ml_ip', 'death_ml_ip', xlim_min = -1, xlim_max = 0.78)\n",
    "# scatterplot(df_final, 'd_pm25_ml_ip', 'death_ml_ip', ylim_min = 0, ylim_max = 200)\n",
    "\n",
    "# 4.1.4.6) Deaths vs dNO2\n",
    "# scatterplot(df_final, 'd_no2_ml', 'death_ml', xlim_min = -1, xlim_max = 1.75)\n",
    "scatterplot(df_final, 'd_no2_ml', 'death_ml', ylim_min = 0, ylim_max = 200)\n",
    "# scatterplot(df_final, 'd_no2_ml_ip', 'death_ml_ip', xlim_min = -1, xlim_max = 1.75)\n",
    "# scatterplot(df_final, 'd_no2_ml_ip', 'death_ml_ip', ylim_min = 0, ylim_max = 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4994b",
   "metadata": {},
   "source": [
    "### 4.1.5) Tokyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.5.1) Deaths vs PM2.5\n",
    "# scatterplot(df_final, 'pm25_tk', 'death_tk', xlim_min = 20, xlim_max = 78)\n",
    "scatterplot(df_final, 'pm25_tk', 'death_tk', ylim_min = 0, ylim_max = 15)\n",
    "# scatterplot(df_final, 'pm25_tk_ip', 'death_tk_ip', xlim_min = 20, xlim_max = 78)\n",
    "# scatterplot(df_final, 'pm25_tk_ip', 'death_tk_ip')\n",
    "\n",
    "# 4.1.5.2) Deaths vs NO2\n",
    "# scatterplot(df_final, 'no2_tk', 'death_tk', xlim_min = 8, xlim_max = 43)\n",
    "scatterplot(df_final, 'no2_tk', 'death_tk', ylim_min = 0, ylim_max = 15)\n",
    "# scatterplot(df_final, 'no2_tk_ip', 'death_tk_ip', xlim_min = 8, xlim_max = 43)\n",
    "# scatterplot(df_final, 'no2_tk_ip', 'death_tk_ip')\n",
    "\n",
    "# 4.1.5.3) Deaths vs lnPM2.5\n",
    "# scatterplot(df_final, 'ln_pm25_tk', 'death_tk', xlim_min = 3, xlim_max = 4.36)\n",
    "scatterplot(df_final, 'ln_pm25_tk', 'death_tk', ylim_min = 0, ylim_max = 15)\n",
    "# scatterplot(df_final, 'ln_pm25_tk_ip', 'death_tk_ip', xlim_min = 3, xlim_max = 4.36)\n",
    "# scatterplot(df_final, 'ln_pm25_tk_ip', 'death_tk_ip')\n",
    "\n",
    "# 4.1.5.4) Deaths vs lnNO2\n",
    "# scatterplot(df_final, 'ln_no2_tk', 'death_tk', xlim_min = 2.08, xlim_max = 3.76)\n",
    "scatterplot(df_final, 'ln_no2_tk', 'death_tk', ylim_min = 0, ylim_max = 15)\n",
    "# scatterplot(df_final, 'ln_no2_tk_ip', 'death_tk_ip', xlim_min = 2.08, xlim_max = 3.76)\n",
    "# scatterplot(df_final, 'ln_no2_tk_ip', 'death_tk_ip')\n",
    "\n",
    "# 4.1.5.5) Deaths vs dPM2.5\n",
    "# scatterplot(df_final, 'd_pm25_tk', 'death_tk', xlim_min = -1, xlim_max = 0.78)\n",
    "scatterplot(df_final, 'd_pm25_tk', 'death_tk', ylim_min = 0, ylim_max = 15)\n",
    "# scatterplot(df_final, 'd_pm25_tk_ip', 'death_tk_ip', xlim_min = -1, xlim_max = 0.78)\n",
    "# scatterplot(df_final, 'd_pm25_tk_ip', 'death_tk_ip')\n",
    "\n",
    "# 4.1.5.6) Deaths vs dNO2\n",
    "# scatterplot(df_final, 'd_no2_tk', 'death_tk', xlim_min = -1, xlim_max = 1.75)\n",
    "scatterplot(df_final, 'd_no2_tk', 'death_tk', ylim_min = 0, ylim_max = 15)\n",
    "# scatterplot(df_final, 'd_no2_tk_ip', 'death_tk_ip', xlim_min = -1, xlim_max = 1.75)\n",
    "# scatterplot(df_final, 'd_no2_tk_ip', 'death_tk_ip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba30eb",
   "metadata": {},
   "source": [
    "## 4.2) Plotting by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80059c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.1) Create functions to obtain ceiling values\n",
    "\n",
    "def round_up_to_nearest_10(value):\n",
    "    return math.ceil(value / 10) * 10\n",
    "\n",
    "def round_up_to_nearest_100(value):\n",
    "    return math.ceil(value / 100) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4eaebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.2) Create dictionary for lockdown start and end dates\n",
    "\n",
    "lock_dict = {'ny': {'start': '2020-03-20', 'end': '2020-06-08'}, \n",
    "             'ml': {'start': '2020-03-09', 'end': '2020-05-18'},\n",
    "             'tk': {'start': '2020-04-07', 'end': '2020-05-25'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393290a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.3) Create a function to plot deaths by date\n",
    "\n",
    "def plot_death_by_date(df, x_column, y_column, lock_start = 'N/A', lock_end = 'N/A'): \n",
    "    \n",
    "    # Create temporary dataframe that removes NAs\n",
    "    df_temp = df.loc[:, [x_column, y_column]].dropna()\n",
    "    \n",
    "    # Find maximum of the y_column\n",
    "    ymax = df_temp[y_column].values.max()\n",
    "    \n",
    "    # Obtain ceiling values using our functions from 4.2.1). If the maximum the of y_column is low, we want multiples of 10, \n",
    "    # otherwise multiples of 100.\n",
    "    if ymax < 50:\n",
    "        ceiling = round_up_to_nearest_10(ymax)\n",
    "    else:\n",
    "        ceiling = round_up_to_nearest_100(ymax)\n",
    "    \n",
    "    # Create an additional value which is half of ceiling value - for the array multiplication below an integer value is needed \n",
    "    half_ceiling = int(ceiling * 0.5) \n",
    "    \n",
    "    plot = px.line(df_temp, x = df_temp[x_column], y = df_temp[y_column])\n",
    "\n",
    "    plot.update_layout(title = f\"Timeline of {y_column}\", xaxis_title = x_column, yaxis_title = y_column,\n",
    "                       legend = dict(yanchor = \"top\", y = 0.99, xanchor = \"right\", x = 0.99))\n",
    "\n",
    "    plot.update_traces(line_color = 'darkcyan', line_width = 3)\n",
    "     \n",
    "    plot.add_trace(go.Scatter(x = df_temp[x_column], y = [ceiling] * len(df_temp), mode = 'lines', \n",
    "                              name = str(ceiling) + ' count line', line = dict(dash = 'dash', color = 'black')))\n",
    "\n",
    "    plot.add_trace(go.Scatter(x = df_temp[x_column], y = [half_ceiling] * len(df_temp), mode = 'lines',\n",
    "                              name = str(half_ceiling) + ' count line', line = dict(dash = 'dash', color = 'maroon')))\n",
    "    \n",
    "    plot.add_trace(go.Scatter(x = [lock_start, lock_start], y = [0, ceiling], mode = 'lines', \n",
    "                             line = dict(color = 'orange', width = 2), name = 'lockdown start: ' + lock_start))\n",
    "    \n",
    "    plot.add_trace(go.Scatter(x = [lock_end, lock_end], y = [0, ceiling], mode = 'lines', \n",
    "                             line = dict(color = 'orange', width = 2), name = 'lockdown end: ' + lock_end))\n",
    "\n",
    "    # Save the plot into the \\figures directory\n",
    "    plot.write_image(f\"..\\\\figures\\_423_ {y_column} by {x_column}.jpg\", format = 'jpg', scale = 1.8)\n",
    "\n",
    "    return(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377cc034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.4) Create a function to plot pollutant concentration by date\n",
    "\n",
    "def plot_poll_by_date(df, x_column, poll1, poll2, lock_start = 'N/A', lock_end = 'N/A'):\n",
    "    \n",
    "    # Create temporary dataframe that removes NAs\n",
    "    df_temp = df.loc[:, [x_column, poll1, poll2]].dropna()\n",
    "    \n",
    "    # Find maximum of the y_column\n",
    "    ymax = df_temp[[poll1, poll2]].values.max()\n",
    "    \n",
    "    plot = px.line(df_temp, x = x_column, y = poll1)\n",
    "\n",
    "    plot.add_trace(go.Scatter(x = df_temp[x_column], y = df_temp[poll1], mode = 'lines', name = poll1, \n",
    "                              line_color = 'darkgreen', line_width = 2))\n",
    "\n",
    "    plot.add_trace(go.Scatter(x = df_temp[x_column], y = df_temp[poll2], mode = 'lines', name = poll2,\n",
    "                              line_color = 'mediumblue', line_width = 2))\n",
    "\n",
    "    plot.update_layout(title = f\"Timeline of {poll1} and {poll2} concentration levels\", xaxis_title = x_column, \n",
    "                       yaxis_title = '$\\\\mu g / m^{3}$', legend = dict(yanchor = \"top\", y = 0.99, xanchor = \"right\", x = 0.99))\n",
    "    \n",
    "    plot.add_trace(go.Scatter(x = [lock_start, lock_start], y = [0, ymax], mode = 'lines', \n",
    "                             line = dict(color = 'orange', width = 2), name = 'lockdown start: ' + lock_start))\n",
    "    \n",
    "    plot.add_trace(go.Scatter(x = [lock_end, lock_end], y = [0, ymax], mode = 'lines', \n",
    "                             line = dict(color = 'orange', width = 2), name = 'lockdown end: ' + lock_end))\n",
    "    \n",
    "    # Save the plot into the \\figures directory\n",
    "    plot.write_image(f\"..\\\\figures\\_424_concentration of {poll1} and {poll2} by {x_column}.jpg\", format = 'jpg', scale = 1.8)\n",
    "    \n",
    "    return(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e388036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.5) Create a function that shows deaths by date and pollutant concentration in a color code\n",
    "\n",
    "def plot_death_vs_poll_by_date(df, x_column, y_column, color_column, lock_start = 'N/A', lock_end = 'N/A'):\n",
    "\n",
    "    # Create temporary dataframe that removes NAs\n",
    "    df_temp = df.loc[:, [x_column, y_column, color_column]].dropna()\n",
    "    \n",
    "    # Find maximum of the y_column\n",
    "    ymax = df_temp[y_column].values.max()\n",
    "    \n",
    "    # Obtain ceiling values using our functions from 4.2.1). If the maximum the of y_column is low, we want multiples of 10, \n",
    "    # otherwise multiples of 100.\n",
    "    if ymax < 50:\n",
    "        ceiling = round_up_to_nearest_10(ymax)\n",
    "    else:\n",
    "        ceiling = round_up_to_nearest_100(ymax)\n",
    "    \n",
    "    plot = px.bar(df_temp.sort_values(y_column, ascending = False),x = x_column, y = y_column, color = color_column, \n",
    "                  title = f\"Timeline of: {y_column} vs {color_column}\", color_discrete_sequence = px.colors.qualitative.Vivid)\n",
    "\n",
    "    plot.update_traces(textposition = 'outside')\n",
    "    \n",
    "    plot.update_layout(uniformtext_minsize = 8, uniformtext_mode = 'hide', xaxis_title = x_column, yaxis_title = y_column, \n",
    "                       showlegend = False)\n",
    "    \n",
    "    plot.add_trace(go.Scatter(x = [lock_start, lock_start], y = [0, ceiling], mode = 'lines', \n",
    "                             line = dict(color = 'orange', width = 2)))\n",
    "    \n",
    "    plot.add_trace(go.Scatter(x = [lock_end, lock_end], y = [0, ceiling], mode = 'lines', \n",
    "                             line = dict(color = 'orange', width = 2)))\n",
    "    \n",
    "    plot.add_annotation(x = lock_start, y = ceiling, text = 'lockdown start: ' + lock_start, showarrow = False, yshift = 10)\n",
    "    \n",
    "    plot.add_annotation(x = lock_end, y = ceiling, text = 'lockdown end: ' + lock_end, showarrow = False, yshift = 10)\n",
    "    \n",
    "    # Save the plot into the \\figures directory\n",
    "    plot.write_image(f\"..\\\\figures\\_425_ {y_column} colored by {color_column} by {x_column}.jpg\", format = 'jpg', scale = 1.8)\n",
    "    \n",
    "    return(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac3d22a",
   "metadata": {},
   "source": [
    "### 4.2.6) New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.6.1) Timeline of confirmed deaths\n",
    "plot_death_by_date(df_final, 'date', 'death_ny', lock_dict.get('ny').get('start'), lock_dict.get('ny').get('end'))\n",
    "# plot_death_by_date(df_final, 'date', 'death_ny_ip', lock_dict.get('ny').get('start'), lock_dict.get('ny').get('end'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c09543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.6.2) Timeline of pollutant concentration levels\n",
    "plot_poll_by_date(df_final, 'date', 'no2_ny', 'pm25_ny', lock_dict.get('ny').get('start'), lock_dict.get('ny').get('end'))\n",
    "# plot_poll_by_date(df_final, 'date', 'no2_ny_ip', 'pm25_ny_ip', lock_dict.get('ny').get('start'), lock_dict.get('ny').get('end'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08861be3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 4.2.6.3) Timeline of death vs pollutant\n",
    "plot_death_vs_poll_by_date(df_final, 'date', 'death_ny', 'pm25_ny', lock_dict.get('ny').get('start'), lock_dict.get('ny').get('end'))\n",
    "# plot_death_vs_poll_by_date(df_final, 'date', 'death_ny_ip', 'pm25_ny_ip', lock_dict.get('ny').get('start'), lock_dict.get('ny').get('end'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c78e6",
   "metadata": {},
   "source": [
    "### 4.2.7) Milan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b7c9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.7.1) Timeline of confirmed deaths\n",
    "plot_death_by_date(df_final, 'date', 'death_ml', lock_dict.get('ml').get('start'), lock_dict.get('ml').get('end'))\n",
    "# plot_death_by_date(df_final, 'date', 'death_ml_ip', lock_dict.get('ml').get('start'), lock_dict.get('ml').get('end'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.7.2) Timeline of pollutant concentration levels\n",
    "plot_poll_by_date(df_final, 'date', 'no2_ml', 'pm25_ml', lock_dict.get('ml').get('start'), lock_dict.get('ml').get('end'))\n",
    "# plot_poll_by_date(df_final, 'date', 'no2_ml_ip', 'pm25_ml_ip', lock_dict.get('ml').get('start'), lock_dict.get('ml').get('end'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ad456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.7.3) Timeline of death vs pollutant\n",
    "plot_death_vs_poll_by_date(df_final, 'date', 'death_ml', 'pm25_ml', lock_dict.get('ml').get('start'), lock_dict.get('ml').get('end'))\n",
    "# plot_death_vs_poll_by_date(df_final, 'date', 'death_ml_ip', 'pm25_ml_ip', lock_dict.get('ml').get('start'), lock_dict.get('ml').get('end'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8af05c1",
   "metadata": {},
   "source": [
    "### 4.2.8) Tokyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.8.1) Timeline of confirmed deaths\n",
    "# plot_death_by_date(df_final, 'date', 'death_tk', lock_dict.get('tk').get('start'), lock_dict.get('tk').get('end'))\n",
    "plot_death_by_date(df_final, 'date', 'death_tk_ip', lock_dict.get('tk').get('start'), lock_dict.get('tk').get('end'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.8.2) Timeline of pollutant concentration levels\n",
    "plot_poll_by_date(df_final, 'date', 'no2_tk', 'pm25_tk', lock_dict.get('tk').get('start'), lock_dict.get('tk').get('end'))\n",
    "# plot_poll_by_date(df_final, 'date', 'no2_tk_ip', 'pm25_tk_ip', lock_dict.get('tk').get('start'), lock_dict.get('tk').get('end'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a1f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.8.3) Timeline of death vs pollutant\n",
    "plot_death_vs_poll_by_date(df_final, 'date', 'death_tk', 'pm25_tk', lock_dict.get('tk').get('start'), lock_dict.get('tk').get('end'))\n",
    "# plot_death_vs_poll_by_date(df_final, 'date', 'death_tk_ip', 'pm25_tk_ip', lock_dict.get('tk').get('start'), lock_dict.get('tk').get('end'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a4bd6",
   "metadata": {},
   "source": [
    "# 5) Prediction with Artificial Neural Networks (ANNs)\n",
    "\n",
    "We are going to use TensorFlow and Keras, the main library for building Neural Networks. The main libraries and modules have already been imported - see 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f468b31",
   "metadata": {},
   "source": [
    "## 5.1 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f11729",
   "metadata": {},
   "source": [
    "We first define the model input columns for all three cities, based on our dataframe df_final. For each city we have one column vector based on the raw variables and another one based on the interpolated variables. To get a better picture what our neural network might produce, we use these column vectors to produce standardized correlation heatmaps. \n",
    "Lastly, as most TensorFlow operations are performed on float values, we use another function to create the input tensors for all three cities, split by raw variables vs. interpolated variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a72247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1.1) For New York\n",
    "# Variables according to Magazzino et al. (2021)\n",
    "input_ny = ['pm25_ny', 'no2_ny',\\\n",
    "            'ln_pm25_ny', 'ln_no2_ny',\\\n",
    "            'd_pm25_ny', 'd_no2_ny',\\\n",
    "            'ln_death_ny', 'd_death_ny', 'death_ny']\n",
    "input_ny_ip = ['pm25_ny_ip', 'no2_ny_ip',\\\n",
    "               'ln_pm25_ny_ip', 'ln_no2_ny_ip',\\\n",
    "               'd_pm25_ny_ip', 'd_no2_ny_ip',\\\n",
    "               'ln_death_ny_ip', 'd_death_ny_ip', 'death_ny_ip']\n",
    "\n",
    "# Reduced set: Excluding log and difference of the death count\n",
    "input_ny_red =['pm25_ny', 'no2_ny',\\\n",
    "               'ln_pm25_ny', 'ln_no2_ny',\\\n",
    "               'd_pm25_ny','d_no2_ny',\\\n",
    "               'death_ny']\n",
    "input_ny_red_ip = ['pm25_ny_ip', 'no2_ny_ip',\\\n",
    "                   'ln_pm25_ny_ip', 'ln_no2_ny_ip',\\\n",
    "                   'd_pm25_ny_ip', 'd_no2_ny_ip',\\\n",
    "                   'death_ny_ip']\n",
    "\n",
    "# Extended set: Adding all available air pollutants\n",
    "input_ny_ext = ['pm25_ny', 'no2_ny', 'o3_ny',\\\n",
    "                'ln_pm25_ny', 'ln_no2_ny', 'ln_o3_ny',\\\n",
    "                'd_pm25_ny', 'd_no2_ny', 'd_o3_ny',\\\n",
    "                'ln_death_ny', 'd_death_ny', 'death_ny']\n",
    "\n",
    "# Note: For New York, we are not including an extended set with interpolated variables, as the interpolation is missing for\n",
    "# the o3_ny pollutant - see 3.3.4)\n",
    "\n",
    "# 5.1.2) For Milan\n",
    "# Variables according to Magazzino et al. (2021)\n",
    "input_ml = ['pm25_ml', 'no2_ml',\\\n",
    "            'ln_pm25_ml', 'ln_no2_ml',\\\n",
    "            'd_pm25_ml', 'd_no2_ml',\\\n",
    "            'ln_death_ml', 'd_death_ml', 'death_ml']\n",
    "input_ml_ip = ['pm25_ml_ip', 'no2_ml_ip',\\\n",
    "               'ln_pm25_ml_ip', 'ln_no2_ml_ip',\\\n",
    "               'd_pm25_ml_ip', 'd_no2_ml_ip',\\\n",
    "               'ln_death_ml_ip', 'd_death_ml_ip', 'death_ml_ip']\n",
    "\n",
    "# Reduced set: Excluding log and difference of the death count\n",
    "input_ml_red = ['pm25_ml', 'no2_ml',\\\n",
    "                'ln_pm25_ml', 'ln_no2_ml',\\\n",
    "                'd_pm25_ml', 'd_no2_ml',\\\n",
    "                'death_ml']\n",
    "input_ml_red_ip = ['pm25_ml_ip', 'no2_ml_ip',\\\n",
    "                   'ln_pm25_ml_ip', 'ln_no2_ml_ip',\\\n",
    "                   'd_pm25_ml_ip', 'd_no2_ml_ip',\\\n",
    "                   'death_ml_ip']\n",
    "\n",
    "# Extended set: Adding all available air pollutants\n",
    "input_ml_ext = ['pm25_ml', 'no2_ml', 'pm10_ml',\\\n",
    "                'ln_pm25_ml', 'ln_no2_ml', 'ln_pm10_ml',\\\n",
    "                'd_pm25_ml','d_no2_ml', 'd_pm10_ml',\\\n",
    "                'ln_death_ml', 'd_death_ml', 'death_ml']\n",
    "input_ml_ext_ip = ['pm25_ml_ip', 'no2_ml_ip', 'pm10_ml_ip',\\\n",
    "                   'ln_pm25_ml_ip', 'ln_no2_ml_ip', 'ln_pm10_ml_ip',\\\n",
    "                   'd_pm25_ml_ip','d_no2_ml_ip', 'd_pm10_ml_ip',\\\n",
    "                   'ln_death_ml_ip', 'd_death_ml_ip', 'death_ml_ip']\n",
    "\n",
    "# 5.1.3) For Tokyo\n",
    "# Variables according to Magazzino et al. (2021)\n",
    "input_tk = ['pm25_tk', 'no2_tk',\\\n",
    "            'ln_pm25_tk', 'ln_no2_tk',\\\n",
    "            'd_pm25_tk','d_no2_tk',\\\n",
    "            'ln_death_tk', 'd_death_tk', 'death_tk']\n",
    "input_tk_ip = ['pm25_tk_ip', 'no2_tk_ip',\\\n",
    "               'ln_pm25_tk_ip', 'ln_no2_tk_ip',\\\n",
    "               'd_pm25_tk_ip', 'd_no2_tk_ip',\\\n",
    "               'ln_death_tk_ip', 'd_death_tk_ip', 'death_tk_ip']\n",
    "\n",
    "# Reduced set: Excluding log and difference of the death count\n",
    "input_tk_red = ['pm25_tk', 'no2_tk',\\\n",
    "                'ln_pm25_tk', 'ln_no2_tk',\\\n",
    "                'd_pm25_tk','d_no2_tk',\\\n",
    "                'death_tk']\n",
    "input_tk_red_ip = ['pm25_tk_ip', 'no2_tk_ip',\\\n",
    "                'ln_pm25_tk_ip', 'ln_no2_tk_ip',\\\n",
    "                'd_pm25_tk_ip','d_no2_tk_ip',\\\n",
    "                'death_tk_ip']\n",
    "\n",
    "# Extended set: Adding all available air pollutants\n",
    "input_tk_ext = ['pm25_tk', 'no2_tk', 'pm10_tk', 'o3_tk', 'so2_tk', 'co_tk',\\\n",
    "                'ln_pm25_tk', 'ln_no2_tk', 'ln_pm10_tk', 'ln_o3_tk', 'ln_so2_tk', 'ln_co_tk',\\\n",
    "                'd_pm25_tk','d_no2_tk', 'd_pm10_tk', 'd_o3_tk', 'd_so2_tk', 'd_co_tk',\\\n",
    "                'ln_death_tk', 'd_death_tk', 'death_tk']\n",
    "input_tk_ext_ip = ['pm25_tk_ip', 'no2_tk_ip', 'pm10_tk_ip', 'o3_tk_ip', 'so2_tk_ip', 'co_tk_ip',\\\n",
    "                'ln_pm25_tk_ip', 'ln_no2_tk_ip', 'ln_pm10_tk_ip', 'ln_o3_tk_ip', 'ln_so2_tk_ip', 'ln_co_tk_ip',\\\n",
    "                'd_pm25_tk_ip', 'd_no2_tk_ip', 'd_pm10_tk_ip', 'd_o3_tk_ip', 'd_so2_tk_ip', 'd_co_tk_ip',\\\n",
    "                'ln_death_tk_ip', 'd_death_tk_ip', 'death_tk_ip']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c7517",
   "metadata": {},
   "source": [
    "## 5.2) Create functions for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c72807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2.1) Create a function to obtain tensors for our X matrix and our y vector - replacing integer values by tf.float64 values\n",
    "\n",
    "def create_X_and_y_tensors(all_columns, y_vector: str, dropna = True):\n",
    "    \n",
    "    if dropna == True:\n",
    "        # Create X and y tensors for the raw variables\n",
    "        df_temp = df_final.loc[:, all_columns].dropna()\n",
    "        all_columns_without_y_vector = [x for x in all_columns if x != y_vector]\n",
    "        X = df_temp.loc[:, all_columns_without_y_vector]\n",
    "        X = tf.convert_to_tensor(X.values, dtype = tf.float64)\n",
    "        y = df_temp.loc[:, y_vector]\n",
    "        y = tf.convert_to_tensor(y.values, dtype = tf.float64)\n",
    "        \n",
    "    else:\n",
    "        # Create X and y tensors for the interpolated variables\n",
    "        df_temp = df_final.loc[:, all_columns]\n",
    "        # Even though we have interpolated variables, we have to drop some 'secondary' NA variables which were added to the dataframe\n",
    "        # when, e.g. taking the logarithm of zero (where zero itself was already the result of interpolation)\n",
    "        all_columns_without_y_vector = [x for x in all_columns if x != y_vector]\n",
    "        X = df_temp.loc[:, all_columns_without_y_vector]\n",
    "        X = tf.convert_to_tensor(X.values, dtype = tf.float64)\n",
    "        y = df_temp_ip.loc[:, y_vector]\n",
    "        y = tf.convert_to_tensor(y.values, dtype = tf.float64)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29381f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2.2) Create a function to split the dataset into the training set and test set\n",
    "\n",
    "def split_train_test(X, y, test_size = 0.2, random_state = 0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.numpy(), y.numpy(), test_size = test_size, random_state = random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132217cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2.3) Create a function to apply feature scaling \n",
    "\n",
    "def apply_feature_scaling(X_train, X_test):\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be3364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2.4) Create a function for the R2 metric (not sure if we should keep that - I switched to MSE below)\n",
    "\n",
    "def R2(y, y_hat):\n",
    "    ss_res =  K.sum(K.square(y - y_hat)) \n",
    "    ss_tot = K.sum(K.square(y - K.mean(y))) \n",
    "    return ( 1 - ss_res/(ss_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25068b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2.6 Create a master train and test function  summarizes all functions in 5.3\n",
    "\n",
    "def master_model_train_test(input_vector, input_vector_name: str, y_vector: str, dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                        optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                        batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1, callbacks = None, verbose = 1):\n",
    "    \n",
    "    print(f\"Training the {input_vector_name} training set: \\n\")\n",
    "    \n",
    "    # Create tensors - output: X, y\n",
    "    X, y = create_X_and_y_tensors(all_columns = input_vector, y_vector = y_vector, dropna = dropna)\n",
    "    number_of_features = X.shape[1]\n",
    "    \n",
    "    # Create Magazzino-Mele-Sarkodie model\n",
    "    # Input layer\n",
    "    inputs = layers.Input(name = \"input\", shape = (number_of_features,))\n",
    "    # Hidden layers\n",
    "    h1 = layers.Dense(name = \"h1\", units = 13, activation = 'relu')(inputs)\n",
    "    h2 = layers.Dense(name = \"h2\", units = 8, activation = 'relu')(h1)\n",
    "    h3 = layers.Dense(name = \"h3\", units = 7, activation = 'relu')(h2)\n",
    "    h4 = layers.Dense(name = \"h4\", units = 8, activation = 'relu')(h3)\n",
    "    h5 = layers.Dense(name = \"h5\", units = 3, activation = 'relu')(h4)\n",
    "    h6 = layers.Dense(name = \"h6\", units = 1, activation = 'relu')(h5)\n",
    "    h7 = layers.Dense(name = \"h7\", units = 1, activation = 'relu')(h6)\n",
    "    # Output layer\n",
    "    outputs = tf.keras.activations.linear(h7)\n",
    "    model = models.Model(inputs = inputs, outputs = outputs, name = \"Magazzino-Mele-Sarkodie\")\n",
    "#     model.summary()\n",
    "\n",
    "    # Split training and test set - output: X_train, X_test, y_train, y_test\n",
    "    X_train, X_test, y_train, y_test = split_train_test(X, y, test_size = test_size, random_state = random_state)\n",
    "    \n",
    "    # Apply feature scaling - output: X_train, X_test\n",
    "    apply_feature_scaling(X_train, X_test)\n",
    "    \n",
    "    # Print train and test shape\n",
    "#     print(\"X_train shape: \", X_train.shape)\n",
    "#     print(\"y_train shape: \", y_train.shape)\n",
    "#     print(\"X_test shape: \", X_test.shape)\n",
    "#     print(\"y_test shape: \", y_test.shape)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer = optimizer, loss = loss, metrics = metrics_input)\n",
    "    \n",
    "    # Train model - output: History of loss and metric\n",
    "    print(\"\\n\")\n",
    "    model_fit = model.fit(x = X_train, y = y_train, batch_size = batch_size, epochs = epochs, shuffle = shuffle, \n",
    "                          validation_split = validation_size, callbacks = callbacks, verbose = verbose)\n",
    "    \n",
    "    # Test model - output: List of loss and metric\n",
    "    print(f\"\\n Testing the {input_vector_name} test set: \\n\") \n",
    "    model_eval = model.evaluate(X_test, y_test) # \n",
    "    \n",
    "    # Update model_dict\n",
    "    train_loss = round(model_fit.history['loss'][-1], 4)\n",
    "    train_metric = round(model_fit.history[metrics_input][-1], 4)\n",
    "    val_loss = round(model_fit.history['val_loss'][-1], 4)\n",
    "    val_metric = round(model_fit.history['val_' + metrics_input][-1], 4)\n",
    "    test_loss = round(model_eval[0], 4)\n",
    "    test_metric = round(model_eval[1], 4)\n",
    "    model_dict.update({input_vector_name: {'train_loss': train_loss, 'val_loss': val_loss, 'test_loss': test_loss,\\\n",
    "                                           'train_metric': train_metric, 'val_metric': val_metric, 'test_metric': test_metric,\\\n",
    "                                           'X_train shape': X_train.shape, 'X_test shape': X_test.shape}})\n",
    "       \n",
    "    metrics = [k for k in model_fit.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]    \n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 2, sharey = True, figsize = (15,3))\n",
    "\n",
    "    # Create training history plot\n",
    "    ax[0].set(title = \"Training history\")   \n",
    "    ax11 = ax[0].twinx()\n",
    "    ax[0].plot(model_fit.history['loss'], color = 'black')   \n",
    "    ax[0].set_xlabel('Epochs')    \n",
    "    ax[0].set_ylabel('Loss', color = 'black')    \n",
    "    for metric in metrics:        \n",
    "        ax11.plot(model_fit.history[metric], label = metric)   \n",
    "        ax11.set_ylabel(\"Score\", color = 'steelblue')    \n",
    "    ax11.legend()\n",
    "\n",
    "    # Create validation history plot\n",
    "    ax[1].set(title = \"Validation history\")    \n",
    "    ax22 = ax[1].twinx()    \n",
    "    ax[1].plot(model_fit.history['val_loss'], color = 'black')   \n",
    "    ax[1].set_xlabel('Epochs')    \n",
    "    ax[1].set_ylabel('Loss', color = 'black')    \n",
    "    for metric in metrics:          \n",
    "        ax22.plot(model_fit.history['val_' + metric], label = metric)   \n",
    "        ax22.set_ylabel(\"Score\", color = \"steelblue\")\n",
    "    \n",
    "     # Save the plot into the \\figures directory\n",
    "    plt.savefig(f\"..\\\\figures\\_526_training_validation_test_.jpg\", dpi = 300, bbox_inches = 'tight')\n",
    "    \n",
    "    return model_fit, model_eval, model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088dde2",
   "metadata": {},
   "source": [
    "## 5.3) Run the model training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e28bce2",
   "metadata": {},
   "source": [
    "### 5.3.1) Create an empty model dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c42531",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aecb014",
   "metadata": {},
   "source": [
    "### 5.3.2) Apply the master_model_train_test() function and update the model dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd8d395",
   "metadata": {},
   "source": [
    "#### 5.3.2.1) New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_ny, input_vector_name = 'input_ny',\\\n",
    "                                            y_vector = 'death_ny', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedde380",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_ny_ip, input_vector_name = 'input_ny_ip',\\\n",
    "                                            y_vector = 'death_ny_ip', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_ny_red, input_vector_name = 'input_ny_red',\\\n",
    "                                            y_vector = 'death_ny', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3397b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_ny_red_ip, input_vector_name = 'input_ny_red_ip',\\\n",
    "                                            y_vector = 'death_ny_ip', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8a7439",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_ny_ext, input_vector_name = 'input_ny_ext',\\\n",
    "                                            y_vector = 'death_ny', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743b4d61",
   "metadata": {},
   "source": [
    "#### 5.3.2.2) Milan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7497d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_ml, input_vector_name = 'input_ml',\\\n",
    "                                            y_vector = 'death_ml', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e711488",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_ml_ip, input_vector_name = 'input_ml_ip',\\\n",
    "                                            y_vector = 'death_ml_ip', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81432de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_ml_red, input_vector_name = 'input_ml_red',\\\n",
    "                                            y_vector = 'death_ml', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f5c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_ml_red_ip, input_vector_name = 'input_ml_red_ip',\\\n",
    "                                            y_vector = 'death_ml_ip', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622e8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_ml_ext, input_vector_name = 'input_ml_ext',\\\n",
    "                                            y_vector = 'death_ml', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57202cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_ml_ext_ip, input_vector_name = 'input_ml_ext_ip',\\\n",
    "                                            y_vector = 'death_ml_ip', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe38f1",
   "metadata": {},
   "source": [
    "#### 5.3.2.3) Tokyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_tk, input_vector_name = 'input_tk',\\\n",
    "                                            y_vector = 'death_tk', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122b4513",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_tk_ip, input_vector_name = 'input_tk_ip',\\\n",
    "                                            y_vector = 'death_tk_ip', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0def88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_tk_red, input_vector_name = 'input_tk_red',\\\n",
    "                                            y_vector = 'death_tk', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca370f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_tk_red_ip, input_vector_name = 'input_tk_red_ip',\\\n",
    "                                            y_vector = 'death_tk_ip', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_tk_ext, input_vector_name = 'input_tk_ext',\\\n",
    "                                            y_vector = 'death_tk', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915c61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model_eval, model_dict = master_model_train_test(input_vector = input_tk_ext_ip, input_vector_name = 'input_tk_ext_ip',\\\n",
    "                                            y_vector = 'death_tk_ip', dropna = True, test_size = 0.2, random_state = 0,\\\n",
    "                                            optimizer = 'adam', loss = 'mean_absolute_error', metrics_input = 'mse',\\\n",
    "                                            batch_size = 100, epochs = 700, shuffle = True, validation_size = 0.1,\\\n",
    "                                            callbacks = None, verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74240602",
   "metadata": {},
   "source": [
    "### 5.3.3) Create a summary table from the model dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a178b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary = pd.DataFrame.from_dict(model_dict, orient = 'index') # orient = 'index': The dictionary keys are used for the rows\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66723e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
